{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Resilient Distributed Dataset* (RDD) est une collection d'objets immuables et distribués. Les RDD sont résilients ou tolérants aux pannes. Les collections d'objets partitionnés sont réparties dans un cluster, stockées en mémoire ou sur disque. Les RDD sont construits et manipulés grâce à un ensemble diversifié de transformations parallèles (*map*, *filter*, *join*) et d'actions (*count*, *collect*, *save*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD peut être créé de plusieurs manières:\n",
    "* Paralléliser une collection\n",
    "* Lire des données à partir d'une source externe\n",
    "* Transformation d'un RDD existant\n",
    "* API de streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD  Operations\n",
    "\n",
    "###  Transformation (Lazy evaluation)\n",
    "Les transformations créent de nouvelles dataset à partir d'une dataset existante. Par exemple, *map* est une transformation qui passe chaque élément de l'ensemble de données via une fonction et renvoie un nouveau RDD représentant les résultats. Toutes les transformations dans Spark sont *lazy*, c'est à dire elles ne calculent pas le résultats automatiquement. Les transformations ne sont calculées que lorsqu'une action nécessite qu'un résultat soit renvoyé.  \n",
    "\n",
    "**Exemple de transformations**\n",
    "\n",
    "*map(),  flatMap(), filter(),  distinct(), intersection(), cartesian(), groupByKey(), coalesce(), mapPartitions(), reduceByKey(), repartition(), sortByKey(), partitionBy(), sample(), ...*  \n",
    "\n",
    "### Actions\n",
    "Les actions permettent de returner la valeur calculer sur le dataset vers le driver. Par exemple, *reduce* est une action qui agrège tous les éléments du RDD en utilisant une fonction et renvoie le résultat final vers le driver.\n",
    "\n",
    "**Exemple d'actions**\n",
    "*reduce(), collect(), count(), first(), take() countByKey(), takeSample(), foreach(), takeOrdered(), saveAsTextFile(), saveAsSequenceFile() ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Dans ce chapitre nous allons faire une introduction des RDD pour vous permettre de comprendre comment fonctionne Spark built-in. Depuis les récentes versions de Spark,les RDD sont désormais considérés dans API de bas niveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parralleliser un collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.115.226:4041\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1659193087151)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(0,1,2,3,4,5,6,7,8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "rdd.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "even: (x: Int)Boolean\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def even(x : Int) = x % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Boolean = true\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "application d'une transformation `filter` sur le RDD retourne une nouvelle RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_filter: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at <console>:26\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_filter = rdd.filter(x => x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_filter: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at filter at <console>:28\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_filter = rdd.filter(even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[Int] = Array(0, 2, 4, 6, 8)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'action `take` sur le RDD permet d'extraire les 5 premiers éléments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[Int] = Array(0, 2, 4, 6, 8)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(x => x % 2 == 0).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = sc.parallelize(Seq(1, 2, 4,5, 9, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[Int] = Array(4, 16)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(x => x * x).filter(x => x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Int = 20\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(x => x * x).filter(x => x % 2 == 0).reduce((a, b) => a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'action `collect` sur le RDD permet d'extraire la liste de tous les éléments. Eviter d'utiliser RDD lorsqu'on a une large volume de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[Int] = Array(0, 2, 4, 6, 8)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la fonction `map` constitue une transformation et permet d'appliquer une fonction en input sur le RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_map: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at <console>:26\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_map = rdd_filter.map(x => x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[Int] = Array(0, 4, 16, 36, 64)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_map.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'action de la fonction `reduce` permet d'appliquer une fonction d'aggregation somme sur le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: Int = 120\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = rdd_map.reduce((a, b) => a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons factoriser le code en créant un *pipeline* de transformation suivi d'une action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_even_sqr: Int = 312960\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sum_even_sqr =  rdd.filter(x => x % 2 == 0).map( x => x*x).map(x => x*x*x).reduce((x,y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture d'un fichier texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.textFile` permet lire un fichier texte et le transforme en RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texte: org.apache.spark.rdd.RDD[String] = datasets/senegal.txt MapPartitionsRDD[5] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val texte= sc.textFile(\"datasets/senegal.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 5\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: String = \"Le Sénégal est un pays situé sur la côte ouest de l'Afrique et doté d'un héritage colonial français et de nombreuses attractions naturelles. \"\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing sur un fichier texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convertion du texte en Majuscule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fistLines: Array[String] = Array(\"LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \", \"DAKAR, LA CAPITALE, COMPREND LE QUARTIER HISTORIQUE DE LA MÉDINA ET LE CÉLÈBRE MUSÉE THÉODORE MONOD, EXPOSANT DES ŒUVRES D'ART AFRICAIN. \")\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fistLines = texte.map(line => line.toUpperCase()).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: String = \"LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \"\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fistLines(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtrage des ligne commençant par L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Sénégal est un pays situé sur la côte ouest de l'Afrique et doté d'un héritage colonial français et de nombreuses attractions naturelles. \n",
      "Le Sénégal est un pays dont le président est démocratiquement élu au suffrage universel directe.\n"
     ]
    }
   ],
   "source": [
    "texte.filter(line => line.startsWith(\"L\")).take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorisation du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LE SÉNÉGAL EST UN PAYS SITUÉ SUR LA CÔTE OUEST DE L'AFRIQUE ET DOTÉ D'UN HÉRITAGE COLONIAL FRANÇAIS ET DE NOMBREUSES ATTRACTIONS NATURELLES. \n",
      "LE SÉNÉGAL EST UN PAYS DONT LE PRÉSIDENT EST DÉMOCRATIQUEMENT ÉLU AU SUFFRAGE UNIVERSEL DIRECTE.\n"
     ]
    }
   ],
   "source": [
    "texte.map(line => line.toUpperCase()).filter(line => line.startsWith(\"L\")).take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap` applique une transformation .split sur le RDD collections pour obtenir la liste des mot contenus dans le texte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[String] = Array(\"Le Sénégal est un pays situé sur la côte ouest de l'Afrique et doté d'un héritage colonial français et de nombreuses attractions naturelles. \", \"Dakar, la capitale, comprend le quartier historique de la Médina et le célèbre musée Théodore Monod, exposant des œuvres d'art africain. \", \"Elle est également réputée pour sa vie nocturne, centrée sur la musique mbalax, originaire du Sénégal. \", \"Saint-Louis, ancienne capitale de l'Afrique-Occidentale française, abrite une vieille ville à l'architecture coloniale. \", Le Sénégal est un pays dont le président est démocratiquement élu au suffrage universel directe.)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[String] = Array(Le, Sénégal, est, un, pays, situé, sur, la, côte, ouest, de, l'Afrique)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Le Sénégal est un pays situé sur la côte ouest de l'Afrique\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[String] = Array(Le, Sénégal, est, un, pays, situé, sur, la, côte, ouest, de, l'Afrique, et, doté, d'un, héritage, colonial, français, et, de, nombreuses, attractions, naturelles., Dakar,, la, capitale,, comprend, le, quartier, historique, de, la, Médina, et, le, célèbre, musée, Théodore, Monod,, exposant, des, œuvres, d'art, africain., Elle, est, également, réputée, pour, sa, vie, nocturne,, centrée, sur, la, musique, mbalax,, originaire, du, Sénégal., Saint-Louis,, ancienne, capitale, de, l'Afrique-Occidentale, française,, abrite, une, vieille, ville, à, l'architecture, coloniale., Le, Sénégal, est, un, pays, dont, le, président, est, démocratiquement, élu, au, suffrage, universel, directe.)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.flatMap(line => line.split(' '))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at distinct at <console>:26\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = texte.flatMap(line => line.split(' ')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à\n",
      "sur\n",
      "française,\n",
      "directe.\n",
      "pour\n",
      "des\n",
      "ouest\n",
      "comprend\n",
      "d'un\n",
      "coloniale.\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capitales: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[17] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val capitales = sc.parallelize(Seq(\"Abuja\",\"Accra\", \"Addis-Abeba\", \"Alger\", \"Dakar\", \"Banjul\", \n",
    "                                   \"Antananarivo\", \"Bamako\", \"Bissau\",\"Caire\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[String] = Array(Abuja, Accra, Addis-Abeba, Alger, Dakar)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitales.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pays: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[18] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pays =  sc.parallelize(Seq(\"Nigeria\", \"Ghana\", \"Éthiopie\", \"Algérie\", \"Sénégal\", \"Gambie\", \n",
    "                               \"Madagascar\",\"Mali\",\"Guinée-Bissau\", \"Égypte\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.zip` permet des generer couples entre chaque pays et son capitale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pays_capitales: org.apache.spark.rdd.RDD[(String, String)] = ZippedPartitionsRDD2[19] at zip at <console>:28\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pays_capitales = pays.zip(capitales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Nigeria,Abuja)\n",
      "(Ghana,Accra)\n",
      "(Éthiopie,Addis-Abeba)\n",
      "(Algérie,Alger)\n",
      "(Sénégal,Dakar)\n",
      "(Gambie,Banjul)\n",
      "(Madagascar,Antananarivo)\n",
      "(Mali,Bamako)\n",
      "(Guinée-Bissau,Bissau)\n",
      "(Égypte,Caire)\n"
     ]
    }
   ],
   "source": [
    " pays_capitales.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le fameux wordcount avec Spark (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_figaro: org.apache.spark.rdd.RDD[String] = datasets/ancien_figaro.txt MapPartitionsRDD[9] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_figaro =  sc.textFile(\"datasets/ancien_figaro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_flatmap: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_flatmap = rdd_figaro.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at map at <console>:26\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_map = rdd_flatmap.map(w => (w, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordcounts = rdd_map.reduceByKey((w1, w2) => w1 + w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(journée,,1)\n",
      "(Ah!,19)\n",
      "(apostrophes,2)\n",
      "(destitué;,1)\n",
      "(budget!,2)\n",
      "(souvent,22)\n",
      "(combinaison,,1)\n",
      "(épigrammes.,2)\n",
      "(plaisir,,2)\n",
      "(resteront,1)\n"
     ]
    }
   ],
   "source": [
    "wordcounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forme condensée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordcounts = rdd_figaro.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(journée,,1)\n",
      "(Ah!,19)\n",
      "(apostrophes,2)\n",
      "(destitué;,1)\n",
      "(budget!,2)\n",
      "(souvent,22)\n",
      "(combinaison,,1)\n",
      "(épigrammes.,2)\n",
      "(plaisir,,2)\n",
      "(resteront,1)\n"
     ]
    }
   ],
   "source": [
    "wordcounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice  \n",
    "\n",
    "Reprendre le wordcount en supprimant les ponctuations, rendre le texte en minuscule, et ordonner le resultat en ordre décroissant des mots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
