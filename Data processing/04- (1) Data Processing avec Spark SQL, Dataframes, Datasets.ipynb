{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing : Spark SQL, Dataframes, Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prérequis\n",
    "\n",
    "* Bonne compréhension du traitement des données à l'aide de Scala.\n",
    "* Cycle de vie du traitement des données\n",
    "   * Lecture de données à partir de fichiers\n",
    "   * Traitement des données à l'aide d'API\n",
    "   * Réécriture des données traitées dans des fichiers\n",
    "* Nous pouvons également utiliser des bases de données comme sources et puits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous aurons un aperçu du cycle de vie du traitement des données.\n",
    "* Lire les données à partir d'un fichier.\n",
    "* Prévisualisez le schéma et les données pour comprendre les caractéristiques des données.\n",
    "* Obtenez un aperçu des API Data Frame ainsi que des fonctions utilisées pour traiter les données.\n",
    "* Vérifiez s'il y a des doublons dans les données.\n",
    "* Obtenez un aperçu sur comment écrire des données se trouvant dans des dataframes dans des fichiers en utilisant des formats de fichier compressès tels que Parquet.\n",
    "* Nous allons approfondir les API Data Frame pour traiter les données dans les notebooks suivants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel sur Spark Context\n",
    "* `SparkSession`est un classe contenue dans le package `org.apache.spark.sql`\n",
    "* Lorsque Spark Application est soumise à l'aide de `spark-submit` ou` spark-shell` ou `pyspark`, un service Web appelé Spark Context est démarré.\n",
    "\n",
    "* Voici un exemple qui montre comment démarrer Spark Shell en locale.\n",
    "```\n",
    "spark-shell \\\n",
    "    --master \"local[*]\"\n",
    "```\n",
    "* Voici un exemple qui montre comment démarrer Spark Shell en mode multinodes dans un cluster.\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0\n",
    "```\n",
    "* **Assurez-vous de bien comprendre l'environnement et utilisez la commande appropriée pour lancer Spark Shell.**\n",
    "* Spark Context maintient le contexte de tous les jobs qui sont soumis jusqu'à ce qu'il soit *'killé'*.\n",
    "* `SparkSession` n'est rien d'autre qu'un wrapper au-dessus de Spark Context.\n",
    "* Nous devons d'abord créer un objet SparkSession avec n'importe quel nom. Mais généralement, nous utilisons «spark». Une fois créé, plusieurs API seront exposées, y compris `read`.\n",
    "* Nous devons au moins définir le nom de l'application et également spécifier le mode d'exécution dans lequel Spark Context doit s'exécuter lors de la création de l'objet `SparkSession`.\n",
    "* Nous pouvons utiliser `appName` pour spécifier le nom de l'application et` master` pour spécifier le mode d'exécution.\n",
    "* Vous trouverez ci-dessous l'exemple d'extrait de code qui démarrera l'objet Spark Session pour nous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.21.226:4043\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1653138395084)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1238dda3\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing\").\n",
    "    master(\"local[8]\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la tab permet de lister la lister des fonctions disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.port,44527)\n",
      "(spark.rdd.compress,True)\n",
      "(spark.repl.class.outputDir,/tmp/tmpzpdgv5ei)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.pyFiles,)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.repl.class.uri,spark://192.168.1.47:44527/classes)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.driver.host,192.168.1.47)\n",
      "(spark.app.id,local-1639821967495)\n",
      "(spark.ui.showConsoleProgress,true)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les APIs de lecture dans Spark\n",
    "\n",
    "Aperçu des API de lecture Spark pour lire des fichiers de différents formats.\n",
    "\n",
    "* `spark` a un tas d'API pour lire les données de fichiers de différents formats.\n",
    "* Toutes les API sont exposées sous `spark.read`\n",
    "     * `text` - pour lire les données d'une seule colonne à partir de fichiers texte ainsi que pour lire chacun des fichiers texte entiers comme un enregistrement.\n",
    "     * `csv`- pour lire les fichiers texte avec des délimiteurs. La valeur par défaut est une virgule, mais nous pouvons également utiliser d'autres délimiteurs.\n",
    "     * `json` - pour lire les données des fichiers JSON\n",
    "     * `orc` - pour lire les données des fichiers ORC\n",
    "     * `parquet` - pour lire les données des fichiers Parquet.\n",
    "     * Nous pouvons également lire les données d'autres formats de fichiers en branchant et en utilisant `spark.read.format`\n",
    "     * Nous pouvons également passer des options basées sur les formats de fichiers. \n",
    "     * `inferSchema` - pour déduire les types de données des colonnes en fonction des données.\n",
    "     * `header` - pour utiliser l'en-tête pour obtenir les noms de colonne dans le cas de fichiers texte.\n",
    "     * `schema` - pour spécifier explicitement le schéma.\n",
    "     \n",
    "* Voyons un exemple sur la façon de lire des données délimitées à partir de fichiers texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68874,2014-07-03 00:00:00.0,1601,COMPLETE\n",
      "68875,2014-07-04 00:00:00.0,10637,ON_HOLD\n",
      "68876,2014-07-06 00:00:00.0,4124,COMPLETE\n",
      "68877,2014-07-07 00:00:00.0,9692,ON_HOLD\n",
      "68878,2014-07-08 00:00:00.0,6753,COMPLETE\n",
      "68879,2014-07-09 00:00:00.0,778,COMPLETE\n",
      "68880,2014-07-13 00:00:00.0,1117,COMPLETE\n",
      "68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT\n",
      "68882,2014-07-22 00:00:00.0,10000,ON_HOLD\n",
      "68883,2014-07-23 00:00:00.0,5533,COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tail datasets/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.read.csv\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    csv(\"datasets/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.read.csv avec option\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    option(\"sep\", \",\").\n",
    "    csv(\"datasets/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.read.format\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    option(\"sep\", \",\").\n",
    "    format(\"csv\").\n",
    "    load(\"datasets/retail_db/orders/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [_c0: int, _c1: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    option(\"inferSchema\", \"True\").\n",
    "    option(\"sep\", \",\").\n",
    "    format(\"csv\").\n",
    "    load(\"datasets/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lecture de données JSON à partir de fichiers texte. Nous pouvons déduire un schéma à partir des données car chaque objet JSON contient à la fois le nom et la valeur de la colonne.\n",
    "* Exemple pour JSON\n",
    "\n",
    "```\n",
    "{ \"order_id\": 1, \"order_date\": \"2013-07-25 00:00:00.0\", \"order_customer_id\": 12345, \"order_status\": \"COMPLETE\" }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.read.json\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    json(\"datasets/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.read.format\n",
    "\n",
    "val orders = spark.\n",
    "    read.\n",
    "    option(\"inferSchema\", \"false\").\n",
    "    schema(\"\"\"order_id INT, order_date TIMESTAMP,\n",
    "              order_customer_id INT, order_status STRING\n",
    "           \"\"\").\n",
    "    format(\"json\").\n",
    "    load(\"datasets/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schéma et Données\n",
    "\n",
    "Voici les API qui peuvent être utilisées pour prévisualiser le schéma et les données.\n",
    "\n",
    "* `printSchema` peut être utilisé pour obtenir les détails du schéma.\n",
    "* `show` peut être utilisé pour prévisualiser les données. Il affichera généralement les 20 premiers enregistrements où la sortie est tronquée.\n",
    "* `describe` peut être utilisé pour extraire des statistiques de nos données.\n",
    "* Nous pouvons transmettre le nombre d'enregistrements et définir truncate sur false lors de la prévisualisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, \n",
    "              order_date STRING, \n",
    "              order_customer_id INT, \n",
    "              order_status STRING\n",
    "           \"\"\"\n",
    "          ).\n",
    "    csv(\"datasets/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Print Schema\n",
    "orders.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------------------+-----------------+---------------+\n",
      "|summary|order_id          |order_date           |order_customer_id|order_status   |\n",
      "+-------+------------------+---------------------+-----------------+---------------+\n",
      "|count  |68883             |68883                |68883            |68883          |\n",
      "|mean   |34442.0           |null                 |6216.571098819738|null           |\n",
      "|stddev |19884.953633337947|null                 |3586.205241263963|null           |\n",
      "|min    |1                 |2013-07-25 00:00:00.0|1                |CANCELED       |\n",
      "|max    |68883             |2014-07-24 00:00:00.0|12435            |SUSPECTED_FRAUD|\n",
      "+-------+------------------+---------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Describe\n",
    "orders.describe().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Preview Data - Default\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130             |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530             |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911             |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657             |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648             |PENDING_PAYMENT|\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Preview Data - 10, with truncate false\n",
    "orders.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'API Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les transformations au niveau de la ligne ou la projection de données peuvent être effectuées à l'aide de `select, selectExpr, withColumn, drop` sur un Dataframe.\n",
    "- Nous pouvons appliquer les fonctions de `org.apache.spark.sql.functions` sur les colonnes en utilisant `select` et `withColumn`\n",
    "- Le filtrage est généralement effectué à l'aide d'un filtre ou sur un Dataframe.\n",
    "- Nous pouvons passer une condition pour filtrer ou en utilisant le style SQL ou le style d'un langage de programmation.\n",
    "- Les agrégations globales peuvent être effectuées directement sur un Dataframe\n",
    "- Les agrégations sont généralement effectuées à l'aide de groupBy, puis les fonctions sont aggrées à l'aide de `agg`.\n",
    "- Le données dans un Data Frame peuven etre triées ou ordonnées en utilisant `sort` ou `orderBy`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudions comment projeter les données en utilisant différentes options telles que `select`, `selectExpr`, `withColumn`, `drop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: List[(Int, String, String, Double, String)] = List((1,Scott,Tiger,1000.0,united states), (2,Henry,Ford,1250.0,India), (3,Nick,Junior,750.0,united KINGDOM), (4,Bill,Gomes,1500.0,AUSTRALIA))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creation du Dataframe employees\n",
    "\n",
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \n",
    "         \"first_name\", \n",
    "         \"last_name\", \n",
    "         \"salary\", \n",
    "         \"nationality\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// projection de first_name et de last_name\n",
    "employeesDF.select(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+\n",
      "|employee_id|first_name|last_name|salary|\n",
      "+-----------+----------+---------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|\n",
      "|          2|     Henry|     Ford|1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|\n",
      "+-----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Projecter tous les données souf Nationality ()\n",
    "employeesDF.drop(\"nationality\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+\n",
      "|employee_id|first_name|last_name|salary|\n",
      "+-----------+----------+---------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|\n",
      "|          2|     Henry|     Ford|1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|\n",
      "+-----------+----------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "empl: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val empl= employeesDF.drop(\"nationality\")\n",
    "empl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF2: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF2 = employeesDF.drop(\"nationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+\n",
      "|employee_id|first_name|last_name|salary|\n",
      "+-----------+----------+---------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|\n",
      "|          2|     Henry|     Ford|1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|\n",
      "+-----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus de details dans le next step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Spark, differentes fonctions existent pour analyser et traiter les données.\n",
    "\n",
    "- Les fonctions sont souvent appliquées sur les valeurs de colonnes.\n",
    "- Les fonctions Spark qui permettent de traiter les colonnes d'un Dataframe sont disponibles dans `org.apache.spark.sql.functions`. Celles-ci sont généralement utilisés dans `select` ou `withColumn` au-dessus du bloc Dataframe\n",
    "-  Il y a environ 300 fonctions prédéfinies disponibles dans Spark. Les plus importantes sont regroupées sous forme de fonctions de manipulation de chaînes de caractères, de manipulation de date, de fonctions numériques et  de fonctions d'agrégation :\n",
    "    \n",
    "    - Fonctions de manipulation de chaînes caractères :\n",
    "        * Concaténation de chaînes - `concat`\n",
    "        * longueur d'une chaine - `length`\n",
    "        * Suppression d'espace debut et fin - `trim, rtrim, ltrim`\n",
    "        * Padding - lpad, rpad\n",
    "        * Extraction de chaînes - `split, substring`\n",
    "    - Fonctions de manipulation de date\n",
    "        * calcul de date - `date_add, date_sub, dateiff, add_months`\n",
    "        * Extraction de date - `dayofmonth, month, year, date_format`\n",
    "        * Obtenir la période de début - `trunc, date_trunc`\n",
    "    - Fonctions numériques - `abs, greatest`\n",
    "    - Fonctions d'agrégation - `sum, min, max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.Column = employee_id\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"employee_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(col(\"employee_id\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+--------------+\n",
      "|employee_id|  full_name|salary|   nationality|\n",
      "+-----------+-----------+------+--------------+\n",
      "|          1|Scott Tiger|1000.0| united states|\n",
      "|          2| Henry Ford|1250.0|         India|\n",
      "|          3|Nick Junior| 750.0|united KINGDOM|\n",
      "|          4| Bill Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+-----------+------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, lit, concat}\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// En utilisant col et lit\n",
    "import org.apache.spark.sql.functions.{col, lit, concat}\n",
    "employeesDF.\n",
    "    select(col(\"employee_id\"),\n",
    "           concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias(\"full_name\"),\n",
    "           col(\"salary\"),\n",
    "           col(\"nationality\")\n",
    "          ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+------------+\n",
      "|employee_id|salary|   nationality|   full_name|\n",
      "+-----------+------+--------------+------------+\n",
      "|          1|1000.0| united states|Scott, Tiger|\n",
      "|          2|1250.0|         India| Henry, Ford|\n",
      "|          3| 750.0|united KINGDOM|Nick, Junior|\n",
      "|          4|1500.0|     AUSTRALIA| Bill, Gomes|\n",
      "+-----------+------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", \n",
    "               concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))).\n",
    "    drop(\"first_name\", \"last_name\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newEmployes: org.apache.spark.sql.DataFrame = [employee_id: int, salary: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newEmployes = employeesDF.\n",
    "    withColumn(\"full_name\", \n",
    "               concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))).\n",
    "    drop(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+------------+\n",
      "|employee_id|salary|   nationality|   full_name|\n",
      "+-----------+------+--------------+------------+\n",
      "|          1|1000.0| united states|Scott, Tiger|\n",
      "|          2|1250.0|         India| Henry, Ford|\n",
      "|          3| 750.0|united KINGDOM|Nick, Junior|\n",
      "|          4|1500.0|     AUSTRALIA| Bill, Gomes|\n",
      "+-----------+------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newEmployes.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4ae4b618\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$\"colname\" equvalent col(\"colname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+------------+\n",
      "|employee_id|salary|   nationality|   full_name|\n",
      "+-----------+------+--------------+------------+\n",
      "|          1|1000.0| united states|Scott, Tiger|\n",
      "|          2|1250.0|         India| Henry, Ford|\n",
      "|          3| 750.0|united KINGDOM|Nick, Junior|\n",
      "|          4|1500.0|     AUSTRALIA| Bill, Gomes|\n",
      "+-----------+------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// En utilisant $ et lit\n",
    "employeesDF.\n",
    "    withColumn(\"full_name\", \n",
    "               concat($\"first_name\", lit(\", \"), $\"last_name\")).\n",
    "    drop(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|function                   |\n",
      "+---------------------------+\n",
      "|!                          |\n",
      "|!=                         |\n",
      "|%                          |\n",
      "|&                          |\n",
      "|*                          |\n",
      "|+                          |\n",
      "|-                          |\n",
      "|/                          |\n",
      "|<                          |\n",
      "|<=                         |\n",
      "|<=>                        |\n",
      "|<>                         |\n",
      "|=                          |\n",
      "|==                         |\n",
      "|>                          |\n",
      "|>=                         |\n",
      "|^                          |\n",
      "|abs                        |\n",
      "|acos                       |\n",
      "|acosh                      |\n",
      "|add_months                 |\n",
      "|aggregate                  |\n",
      "|and                        |\n",
      "|any                        |\n",
      "|approx_count_distinct      |\n",
      "|approx_percentile          |\n",
      "|array                      |\n",
      "|array_contains             |\n",
      "|array_distinct             |\n",
      "|array_except               |\n",
      "|array_intersect            |\n",
      "|array_join                 |\n",
      "|array_max                  |\n",
      "|array_min                  |\n",
      "|array_position             |\n",
      "|array_remove               |\n",
      "|array_repeat               |\n",
      "|array_sort                 |\n",
      "|array_union                |\n",
      "|arrays_overlap             |\n",
      "|arrays_zip                 |\n",
      "|ascii                      |\n",
      "|asin                       |\n",
      "|asinh                      |\n",
      "|assert_true                |\n",
      "|atan                       |\n",
      "|atan2                      |\n",
      "|atanh                      |\n",
      "|avg                        |\n",
      "|base64                     |\n",
      "|between                    |\n",
      "|bigint                     |\n",
      "|bin                        |\n",
      "|binary                     |\n",
      "|bit_and                    |\n",
      "|bit_count                  |\n",
      "|bit_length                 |\n",
      "|bit_or                     |\n",
      "|bit_xor                    |\n",
      "|bool_and                   |\n",
      "|bool_or                    |\n",
      "|boolean                    |\n",
      "|bround                     |\n",
      "|cardinality                |\n",
      "|case                       |\n",
      "|cast                       |\n",
      "|cbrt                       |\n",
      "|ceil                       |\n",
      "|ceiling                    |\n",
      "|char                       |\n",
      "|char_length                |\n",
      "|character_length           |\n",
      "|chr                        |\n",
      "|coalesce                   |\n",
      "|collect_list               |\n",
      "|collect_set                |\n",
      "|concat                     |\n",
      "|concat_ws                  |\n",
      "|conv                       |\n",
      "|corr                       |\n",
      "|cos                        |\n",
      "|cosh                       |\n",
      "|cot                        |\n",
      "|count                      |\n",
      "|count_if                   |\n",
      "|count_min_sketch           |\n",
      "|covar_pop                  |\n",
      "|covar_samp                 |\n",
      "|crc32                      |\n",
      "|cube                       |\n",
      "|cume_dist                  |\n",
      "|current_database           |\n",
      "|current_date               |\n",
      "|current_timestamp          |\n",
      "|date                       |\n",
      "|date_add                   |\n",
      "|date_format                |\n",
      "|date_part                  |\n",
      "|date_sub                   |\n",
      "|date_trunc                 |\n",
      "|datediff                   |\n",
      "|day                        |\n",
      "|dayofmonth                 |\n",
      "|dayofweek                  |\n",
      "|dayofyear                  |\n",
      "|decimal                    |\n",
      "|decode                     |\n",
      "|degrees                    |\n",
      "|dense_rank                 |\n",
      "|div                        |\n",
      "|double                     |\n",
      "|e                          |\n",
      "|element_at                 |\n",
      "|elt                        |\n",
      "|encode                     |\n",
      "|every                      |\n",
      "|exists                     |\n",
      "|exp                        |\n",
      "|explode                    |\n",
      "|explode_outer              |\n",
      "|expm1                      |\n",
      "|extract                    |\n",
      "|factorial                  |\n",
      "|filter                     |\n",
      "|find_in_set                |\n",
      "|first                      |\n",
      "|first_value                |\n",
      "|flatten                    |\n",
      "|float                      |\n",
      "|floor                      |\n",
      "|forall                     |\n",
      "|format_number              |\n",
      "|format_string              |\n",
      "|from_csv                   |\n",
      "|from_json                  |\n",
      "|from_unixtime              |\n",
      "|from_utc_timestamp         |\n",
      "|get_json_object            |\n",
      "|greatest                   |\n",
      "|grouping                   |\n",
      "|grouping_id                |\n",
      "|hash                       |\n",
      "|hex                        |\n",
      "|hour                       |\n",
      "|hypot                      |\n",
      "|if                         |\n",
      "|ifnull                     |\n",
      "|in                         |\n",
      "|initcap                    |\n",
      "|inline                     |\n",
      "|inline_outer               |\n",
      "|input_file_block_length    |\n",
      "|input_file_block_start     |\n",
      "|input_file_name            |\n",
      "|instr                      |\n",
      "|int                        |\n",
      "|isnan                      |\n",
      "|isnotnull                  |\n",
      "|isnull                     |\n",
      "|java_method                |\n",
      "|json_tuple                 |\n",
      "|kurtosis                   |\n",
      "|lag                        |\n",
      "|last                       |\n",
      "|last_day                   |\n",
      "|last_value                 |\n",
      "|lcase                      |\n",
      "|lead                       |\n",
      "|least                      |\n",
      "|left                       |\n",
      "|length                     |\n",
      "|levenshtein                |\n",
      "|like                       |\n",
      "|ln                         |\n",
      "|locate                     |\n",
      "|log                        |\n",
      "|log10                      |\n",
      "|log1p                      |\n",
      "|log2                       |\n",
      "|lower                      |\n",
      "|lpad                       |\n",
      "|ltrim                      |\n",
      "|make_date                  |\n",
      "|make_interval              |\n",
      "|make_timestamp             |\n",
      "|map                        |\n",
      "|map_concat                 |\n",
      "|map_entries                |\n",
      "|map_filter                 |\n",
      "|map_from_arrays            |\n",
      "|map_from_entries           |\n",
      "|map_keys                   |\n",
      "|map_values                 |\n",
      "|map_zip_with               |\n",
      "|max                        |\n",
      "|max_by                     |\n",
      "|md5                        |\n",
      "|mean                       |\n",
      "|min                        |\n",
      "|min_by                     |\n",
      "|minute                     |\n",
      "|mod                        |\n",
      "|monotonically_increasing_id|\n",
      "|month                      |\n",
      "|months_between             |\n",
      "|named_struct               |\n",
      "|nanvl                      |\n",
      "|negative                   |\n",
      "|next_day                   |\n",
      "|not                        |\n",
      "|now                        |\n",
      "|ntile                      |\n",
      "|nullif                     |\n",
      "|nvl                        |\n",
      "|nvl2                       |\n",
      "|octet_length               |\n",
      "|or                         |\n",
      "|overlay                    |\n",
      "|parse_url                  |\n",
      "|percent_rank               |\n",
      "|percentile                 |\n",
      "|percentile_approx          |\n",
      "|pi                         |\n",
      "|pmod                       |\n",
      "|posexplode                 |\n",
      "|posexplode_outer           |\n",
      "|position                   |\n",
      "|positive                   |\n",
      "|pow                        |\n",
      "|power                      |\n",
      "|printf                     |\n",
      "|quarter                    |\n",
      "|radians                    |\n",
      "|rand                       |\n",
      "|randn                      |\n",
      "|random                     |\n",
      "|rank                       |\n",
      "|reflect                    |\n",
      "|regexp_extract             |\n",
      "|regexp_replace             |\n",
      "|repeat                     |\n",
      "|replace                    |\n",
      "|reverse                    |\n",
      "|right                      |\n",
      "|rint                       |\n",
      "|rlike                      |\n",
      "|rollup                     |\n",
      "|round                      |\n",
      "|row_number                 |\n",
      "|rpad                       |\n",
      "|rtrim                      |\n",
      "|schema_of_csv              |\n",
      "|schema_of_json             |\n",
      "|second                     |\n",
      "|sentences                  |\n",
      "|sequence                   |\n",
      "|sha                        |\n",
      "|sha1                       |\n",
      "|sha2                       |\n",
      "|shiftleft                  |\n",
      "|shiftright                 |\n",
      "|shiftrightunsigned         |\n",
      "|shuffle                    |\n",
      "|sign                       |\n",
      "|signum                     |\n",
      "|sin                        |\n",
      "|sinh                       |\n",
      "|size                       |\n",
      "|skewness                   |\n",
      "|slice                      |\n",
      "|smallint                   |\n",
      "|some                       |\n",
      "|sort_array                 |\n",
      "|soundex                    |\n",
      "|space                      |\n",
      "|spark_partition_id         |\n",
      "|split                      |\n",
      "|sqrt                       |\n",
      "|stack                      |\n",
      "|std                        |\n",
      "|stddev                     |\n",
      "|stddev_pop                 |\n",
      "|stddev_samp                |\n",
      "|str_to_map                 |\n",
      "|string                     |\n",
      "|struct                     |\n",
      "|substr                     |\n",
      "|substring                  |\n",
      "|substring_index            |\n",
      "|sum                        |\n",
      "|tan                        |\n",
      "|tanh                       |\n",
      "|timestamp                  |\n",
      "|tinyint                    |\n",
      "|to_csv                     |\n",
      "|to_date                    |\n",
      "|to_json                    |\n",
      "|to_timestamp               |\n",
      "|to_unix_timestamp          |\n",
      "|to_utc_timestamp           |\n",
      "+---------------------------+\n",
      "only showing top 300 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW functions\").show(300, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                             |\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|Function: concat                                                                          |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.Concat                                   |\n",
      "|Usage: concat(col1, col2, ..., colN) - Returns the concatenation of col1, col2, ..., colN.|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION concat\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+--------------+\n",
      "|employee_id|  full_name|salary|   nationality|\n",
      "+-----------+-----------+------+--------------+\n",
      "|          1|Scott Tiger|1000.0| united states|\n",
      "|          2| Henry Ford|1250.0|         India|\n",
      "|          3|Nick Junior| 750.0|united KINGDOM|\n",
      "|          4| Bill Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+-----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// En utilisant Style SQL \n",
    "employeesDF.\n",
    "    selectExpr(\"employee_id\",\n",
    "               \"concat(first_name, ' ', last_name) AS full_name\",\n",
    "               \"salary\", \n",
    "               \"nationality\"\n",
    "              ).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vue d'ensemble sur les APIs de Lecture/écriture de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les APIs sont exposés par `spark.read`.\n",
    "- **txt** - pour écrire des données à une seule colonne dans des fichiers texte.\n",
    "- **csv** - pour écrire dans des fichiers texte avec des délimiteurs. La valeur par défaut est une virgule, mais nous pouvons également utiliser d'autres délimiteurs.\n",
    "- **json** - pour écrire des données dans des fichiers JSON\n",
    "- **orc** - pour écrire des données dans des fichiers ORC\n",
    "- **parquet** - pour écrire des données dans des fichiers Parquet.\n",
    "\n",
    " Nous pouvons également écrire des données dans d'autres formats de fichiers en branchant et en utilisant `write.format `, par exemple **avro**.  \n",
    " Nous pouvons utiliser des options basées sur le type dans lequel nous écrivons le Dataframe.\n",
    " - **compression** - Codec de compression (gzip, snappy, etc.)\n",
    " - **sep** - pour spécifier des délimiteurs lors de l'écriture dans des fichiers texte en utilisant csv\n",
    " \n",
    " Nous pouvons écraser les répertoires ou les ajouter aux répertoires existants en utilisant `mode`.\n",
    " Créez une copie des données de commande au format de fichier parquet sans compression. Si le dossier existe déjà, écrasez-le. Emplacement cible: `/user/[YOUR_USER_NAME]/retail_db/orders`  \n",
    " Par défaut, le nombre de fichiers dans le répertoire de sortie est égal au nombre de tâches utilisées pour traiter les données à la dernière étape. Cependant, nous pourrions vouloir contrôler le nombre de fichiers afin de ne pas nous heurter à un trop grand nombre de petits fichiers.  \n",
    "Nous pouvons contrôler le nombre de fichiers en utilisant `coalesce`. Il doit être appelé au-dessus du Dataframe avant d'appeler `write`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, \n",
    "              order_date STRING, \n",
    "              order_customer_id INT, \n",
    "              order_status STRING\n",
    "           \"\"\"\n",
    "          ).\n",
    "    csv(\"datasets/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: String = snappy\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.parquet.compression.codec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "// En utilisant write.parquet\n",
    "orders.\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    parquet(\"datasets/training/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "// En utilisant write.format(\"parquet\")\n",
    "orders.\n",
    "    coalesce(1).\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    format(\"parquet\").\n",
    "    save(\"datasets/training/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vous pouvez excuter cette commande dans un cluster hadoop pour lister les chuncks crées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"hdfs dfs -ls  datasets/training/retail_db/orders\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"datasets/training/retail_db/orders\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"datasets/training/retail_db/orders\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
