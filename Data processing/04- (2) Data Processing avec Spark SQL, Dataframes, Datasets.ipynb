{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing : Spark SQL, Dataframes, Datasets \n",
    "suite (3) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processsing avec Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.115.226:4042\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1659193881311)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5860b713\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing\").\n",
    "    master(\"local[8]\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une `case class` pour définir la structure du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(id: Integer, firstName: String, middleName: String, lastName: String, gender: String, birthDate: String, ssn: String, salary: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,firstName,middleName,lastName,gender,birthDate,ssn,salary\n",
      "1,Fanchon,Georgeta,Wylma,Female,5/20/2020,537-80-6230,81273.63\n",
      "2,Sandor,Bordie,Humbert,Male,3/12/2020,640-38-2361,11516.93\n",
      "3,Quillan,Ara,Hillery,Male,2/3/2020,647-45-5964,37622.42\n",
      "4,Tallie,Artemis,Urbain,Male,2/13/2020,714-11-1463,63124.83\n",
      "5,Katharine,Elicia,Muire,Female,2/28/2020,869-39-9071,44189.54\n",
      "6,Lavinie,Daffie,Xylia,Female,9/9/2020,280-89-8466,48111.47\n",
      "7,Efrem,Emlen,Beau,Male,4/12/2020,533-59-2497,94346.03\n",
      "8,Emmaline,Audy,Dorothee,Female,7/14/2020,356-96-4442,73595.17\n",
      "9,King,Gaston,Jaimie,Male,1/27/2020,239-18-8955,5656.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head datasets/people-with-header-10m.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personDS: org.apache.spark.sql.Dataset[Person] = [id: int, firstName: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personDS = spark\n",
    "  .read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .csv(\"datasets/people-with-header-10m.csv\")\n",
    "  .as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.Dataset[Person] = [id: int, firstName: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "| id|firstName|middleName|lastName|gender|birthDate|        ssn|  salary|\n",
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "|  1|  Fanchon|  Georgeta|   Wylma|Female|5/20/2020|537-80-6230|81273.63|\n",
      "|  2|   Sandor|    Bordie| Humbert|  Male|3/12/2020|640-38-2361|11516.93|\n",
      "|  3|  Quillan|       Ara| Hillery|  Male| 2/3/2020|647-45-5964|37622.42|\n",
      "|  4|   Tallie|   Artemis|  Urbain|  Male|2/13/2020|714-11-1463|63124.83|\n",
      "|  5|Katharine|    Elicia|   Muire|Female|2/28/2020|869-39-9071|44189.54|\n",
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personDS.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrage de personnes dont le prenom est `Joe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "| id|firstName|middleName|lastName|gender|birthDate|        ssn|  salary|\n",
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "|887|      Joe|    Seumas|     Lon|  Male|1/20/2020|842-80-5339| 33660.7|\n",
      "| 90|      Joe| Aleksandr|  Goober|  Male| 3/4/1960|236-59-8313|92835.04|\n",
      "| 69|      Joe|     Dolph|   Grady|  Male|1/24/1980|850-35-5174| 69746.9|\n",
      "|355|      Joe|    Gaston| Thadeus|  Male| 4/3/1990|479-07-7410| 66034.1|\n",
      "+---+---------+----------+--------+------+---------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personDS.filter($\"firstName\" === \"Joe\").distinct().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Api\n",
    "// Coompter le nombre de Personne dont le prenom est Joe\n",
    "println(personDS.filter($\"firstName\" === \"Joe\").distinct().count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "// Compter le nombre de Personne dont le prenom est Joe\n",
    "println(personDS.filter(col(\"firstName\") === \"Joe\").distinct().count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrage dees personnes agées de 40 ans, qui ont un salaire superieur à 1000$ dont le prénom commence par J et le nom par M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthDate: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personDS.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date: String = 1/20/2020\n",
       "res29: Boolean = true\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val date = \"1/20/2020\"\n",
    "date.split(\"/\")(2).toInt > 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|        ssn|  salary|\n",
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "|  1|  Fanchon|  Georgeta|   Wylma|Female| 5/20/2020|537-80-6230|81273.63|\n",
      "|  2|   Sandor|    Bordie| Humbert|  Male| 3/12/2020|640-38-2361|11516.93|\n",
      "|  3|  Quillan|       Ara| Hillery|  Male|  2/3/2020|647-45-5964|37622.42|\n",
      "|  4|   Tallie|   Artemis|  Urbain|  Male| 2/13/2020|714-11-1463|63124.83|\n",
      "|  5|Katharine|    Elicia|   Muire|Female| 2/28/2020|869-39-9071|44189.54|\n",
      "|  6|  Lavinie|    Daffie|   Xylia|Female|  9/9/2020|280-89-8466|48111.47|\n",
      "|  7|    Efrem|     Emlen|    Beau|  Male| 4/12/2020|533-59-2497|94346.03|\n",
      "|  8| Emmaline|      Audy|Dorothee|Female| 7/14/2020|356-96-4442|73595.17|\n",
      "|  9|     King|    Gaston|  Jaimie|  Male| 1/27/2020|239-18-8955| 5656.61|\n",
      "| 11|  Agustin|   Ferrell| Chaunce|  Male| 6/18/2020|172-53-1153|45295.43|\n",
      "| 12| Clerissa|     Wilie|   Leone|Female| 9/15/2020|750-94-5487|33928.07|\n",
      "| 13|    Tobie|  Sullivan|  Gasper|  Male| 3/27/2020|516-17-7750|93825.79|\n",
      "| 14|    Tommy|     Ervin|   Nikki|  Male|  7/8/2020|245-82-0539|82578.55|\n",
      "| 15|  Sampson|     Jorge|    Chic|  Male| 8/15/2020|185-58-9259|31507.68|\n",
      "| 16| Mallissa|  La verne|Eleanora|Female|  9/3/2020|413-15-7812|61681.69|\n",
      "| 17|     Rhys|     Arney| Gustavo|  Male| 3/14/2020|744-93-3629| 9586.85|\n",
      "| 18|  Delbert|   Felicio|   Chico|  Male| 8/26/2020|425-19-5864|49640.02|\n",
      "| 19| Ernaline|    Drusie| Annetta|Female|  7/4/2020|245-80-3821|86634.45|\n",
      "| 20|   Alexio|   Walther|    Eddy|  Male| 9/15/2020|799-78-3232|14988.16|\n",
      "| 21|      Adi|     Maire|   Jamie|Female|11/30/2020|665-07-5715|10095.53|\n",
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personDS\n",
    "  .filter (split($\"birthDate\", \"/\")(2).cast(\"int\") === 2020)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\n",
       "earliestYear: Int = 1992\n",
       "res6: Int = 1992\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Calendar\n",
    "val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 30\n",
    "earliestYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|        ssn|  salary|\n",
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "| 57|    Josie|     Mirna| Chelsae|Female| 9/18/2020|258-93-0816|29996.88|\n",
      "| 60| Julianne|   Celinka| Clarita|Female|  4/6/2020|831-14-9854|79505.56|\n",
      "| 61|     Jean|      Garv|   Zelig|  Male| 12/1/2020|134-16-0901|92996.35|\n",
      "| 82|     Jock|      Arie| Herbert|  Male|  1/7/2020|219-82-1860|44868.01|\n",
      "| 85|   Jamima|     Dasha| Rosanne|Female| 6/14/2020|329-53-9510|31357.74|\n",
      "|133| Jessamyn|    Ashlie| Vitoria|Female| 12/7/2020|720-94-5366| 45905.3|\n",
      "|150|   Jammie|   Moselle|   Jandy|Female|12/24/2019|279-44-4635|45918.75|\n",
      "|151|    Jared|     Dilan|   Doyle|  Male|  1/6/2020|222-95-8092|93273.17|\n",
      "|167|    Jodie|  Nicolais|  Byrann|  Male|11/15/2020|134-86-4614|76718.58|\n",
      "|169|     Jena|      Viva|   Ebony|Female| 11/5/2020|639-30-6092|49147.26|\n",
      "|176|   Jdavie|   Purcell|  Siffre|  Male| 3/23/2020|832-51-1831|54786.28|\n",
      "|184|    Jemmy|     Jerry|  Fawnia|Female| 4/25/2020|772-61-9441|40543.68|\n",
      "|194|  Jillana|    Brigid|   Shari|Female| 4/16/2020|663-92-9838|33174.18|\n",
      "|214|  Jessika|  Dorothee| Daphene|Female| 3/13/2020|735-35-3770|32326.11|\n",
      "|225|  Josepha|   Dierdre| Karleen|Female| 10/8/2020|790-85-6054|41573.48|\n",
      "|247|   Jerald|    D'arcy|  Newton|  Male| 1/28/2020|483-50-1590|53771.04|\n",
      "|281|   Jackie|        Jo| Brannon|  Male| 8/15/2020|691-76-6964|56793.89|\n",
      "|296|   Judith|      Dale|   Celka|Female|  6/1/2020|715-45-5543|87911.69|\n",
      "|312|    Jerry|    Kipper|  Yancey|  Male|  4/2/2020|814-79-9566|51326.92|\n",
      "|377|     Jody|    Gerald| Richard|  Male| 3/14/2020|773-94-8880| 97452.3|\n",
      "+---+---------+----------+--------+------+----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import java.util.Calendar\n",
       "earliestYear: Int = 1991\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// definition de la date de naissance des quatergènaire.\n",
    "import java.util.Calendar\n",
    "val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 30\n",
    "\n",
    "personDS\n",
    "  .filter(split($\"birthDate\", \"/\")(2).cast(\"int\") > earliestYear) // everyone above 30\n",
    "  .filter($\"salary\" > 8000.0) // everyone earning more than 80K\n",
    "  .filter($\"firstName\".startsWith(\"J\")) // first name starts with J\n",
    ".show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import java.util.Calendar\n",
       "earliestYear: Int = 1991\n",
       "res49: Long = 45\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// definition de la date de naissance des quatergènaire.\n",
    "import java.util.Calendar\n",
    "val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 30\n",
    "\n",
    "personDS\n",
    "  .filter(split($\"birthDate\", \"/\")(2).cast(\"int\") > earliestYear) // everyone above 40\n",
    "  .filter($\"salary\" > 8000.0) // everyone earning more than 80K\n",
    "  .filter($\"firstName\".startsWith(\"J\")) // first name starts with J\n",
    "  .count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing avec des données tabulaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5c43b1af\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Preprocesing column data\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l: List[String] = List(X)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Vars: string]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"Vars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Vars: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Vars|\n",
      "+----+\n",
      "|   X|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[41] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [value: int]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = rdd.toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le DataFrame créé, nous pouvons l'utiliser pour comprendre comment utiliser les fonctions. Par exemple, pour obtenir la date actuelle, nous pouvons exécuter `df.select (current_date ()).show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.current_date\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Date courant|\n",
      "+------------+\n",
      "|  2022-05-21|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date as \"Date courant\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: List[(Int, String, String, Double, String, String, String)] = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,united KINGDOM,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,AUSTRALIA,+61 987 654 3210,789 12 6118), (5,Gary,Smith,1500.0,Australia,+61 587 656 3280,789 12 6118))\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     ),\n",
    "                     (5, \"Gary\", \"Smith\", 1500.0, \n",
    "                      \"Australia\", \"+61 587 656 3280\", \"789 12 6118\")\n",
    "                     \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Int = 5\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.createTempView(\"Employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "| united states|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "|     Australia|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select nationality, count(1) as count from Employee group by nationality\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|Scott - Tiger|\n",
      "| Henry - Ford|\n",
      "|Nick - Junior|\n",
      "| Bill - Gomes|\n",
      "| Gary - Smith|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select concat(first_name,' - ',last_name) as name from Employee\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n",
      "|5          |Gary      |Smith    |1500.0|Australia     |+61 587 656 3280|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "|      Gary|    Smith|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation des fonctions Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Pour l'utilisation de $ dans les functions à la place de col\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "|      Gary|    Smith|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"first_name\", $\"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Pour l'utilisation de col\n",
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "|      Gary|    Smith|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Utilisation de col \n",
    "\n",
    "employeesDF.\n",
    "    select(col(\"first_name\"), $\"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "|      Gary|    Smith|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Utilisation du nom des colonnes.\n",
    "employeesDF.\n",
    "    select(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "| united states|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "|     Australia|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    groupBy(\"nationality\").\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.upper\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    2|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    groupBy(upper($\"nationality\")).\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select\n",
    ".filter\n",
    ".where\n",
    ".groupBy\n",
    ".orderBy\n",
    ".count\n",
    ".agg\n",
    ".join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orderBy colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          5|      Gary|    Smith|1500.0|     Australia|+61 587 656 3280|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    orderBy(asc(\"employee_id\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, si nous voulons appliquer une transformation à l'aide de fonctions, passer des noms de colonnes sous forme de chaînes à certaines des fonctions ne suffira pas. Nous devons les passer comme type de colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.desc\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          5|      Gary|    Smith|1500.0|     Australia|+61 587 656 3280|789 12 6118|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    orderBy(desc(\"first_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          5|      Gary|    Smith|1500.0|     Australia|+61 587 656 3280|789 12 6118|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    orderBy(col(\"employee_id\").desc).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "49: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:49: error: type mismatch;",
      " found   : String(\"first_name\")",
      " required: org.apache.spark.sql.Column",
      "           select(upper(\"first_name\")).",
      "                        ^",
      ""
     ]
    }
   ],
   "source": [
    "//Error\n",
    "employeesDF.\n",
    "    select(lower(\"first_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.upper\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|upper(first_name)|\n",
      "+-----------------+\n",
      "|            SCOTT|\n",
      "|            HENRY|\n",
      "|             NICK|\n",
      "|             BILL|\n",
      "|             GARY|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// version correcte\n",
    "employeesDF.\n",
    "    select(upper(col(\"first_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|upper(first_name)|\n",
      "+-----------------+\n",
      "|            SCOTT|\n",
      "|            HENRY|\n",
      "|             NICK|\n",
      "|             BILL|\n",
      "|             GARY|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// version alternative\n",
    "employeesDF.\n",
    "    select(upper($\"first_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    2|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dans le cas de groupBy\n",
    "employeesDF.\n",
    "    groupBy(upper($\"nationality\")).\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          5|      Gary|    Smith|1500.0|     Australia|+61 587 656 3280|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Dans le cas de ordeBy\n",
    "employeesDF.\n",
    "    orderBy(upper($\"nationality\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// version alternative\n",
    "employeesDF.\n",
    "    orderBy(upper(employeesDF(\"nationality\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.sql.Column = first_name\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF(\"first_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{concat, lit}\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{concat,lit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions incorrectes de concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: type mismatch;",
      " found   : String(\", \")",
      " required: org.apache.spark.sql.Column",
      "           select(concat($\"first_name\", \", \", $\"last_name\")).",
      "                                        ^",
      ""
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", \", \", $\"last_name\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "100: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:100: error: type mismatch;",
      " found   : String(\", \")",
      " required: org.apache.spark.sql.Column",
      "           select(concat(col(\"first_name\"), \", \", col(\"last_name\"))).",
      "                                            ^",
      ""
     ]
    }
   ],
   "source": [
    "// Deme\n",
    "employeesDF.\n",
    "    select(concat(col(\"first_name\"), \", \", col(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "99: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:99: error: type mismatch;",
      " found   : String(\", \")",
      " required: org.apache.spark.sql.Column",
      "           select(concat(employeesDF(\"first_name\"), \", \", employeesDF(\"last_name\"))).",
      "                                                    ^",
      ""
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat(employeesDF(\"first_name\"), \", \", employeesDF(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "51: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:51: error: type mismatch;",
      " found   : String(\"first_name\")",
      " required: org.apache.spark.sql.Column",
      "           select(concat(\"first_name\", \", \", \"last_name\")).",
      "                         ^",
      "<console>:51: error: type mismatch;",
      " found   : String(\", \")",
      " required: org.apache.spark.sql.Column",
      "           select(concat(\"first_name\", \", \", \"last_name\")).",
      "                                       ^",
      "<console>:51: error: type mismatch;",
      " found   : String(\"last_name\")",
      " required: org.apache.spark.sql.Column",
      "           select(concat(\"first_name\", \", \", \"last_name\")).",
      "                                             ^",
      ""
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat(\"first_name\", \", \", \"last_name\")).\n",
    "    show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "versions correctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{concat, col, lit}\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{concat, col, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|    Full Name|Status|\n",
      "+-------------+------+\n",
      "|Scott - Tiger|   yes|\n",
      "| Henry - Ford|   yes|\n",
      "|Nick - Junior|   yes|\n",
      "| Bill - Gomes|   yes|\n",
      "| Gary - Smith|   yes|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat(col(\"first_name\"), lit(\" - \"), col(\"last_name\")) as \"Full Name\").\n",
    "    withColumn(\"Status\", lit(\"yes\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|concat(first_name, , , last_name)|\n",
      "+---------------------------------+\n",
      "|                     Scott, Tiger|\n",
      "|                      Henry, Ford|\n",
      "|                     Nick, Junior|\n",
      "|                      Bill, Gomes|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", lit(\", \"), employeesDF(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation de chaines de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: List[(Int, String, String, Double, String, String, String)] = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,united KINGDOM,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,AUSTRALIA,+61 987 654 3210,789 12 6118))\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, upper, lower, initcap, length}\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper, lower, initcap, length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"nationality\").\n",
    "    withColumn(\"nationality_upper\", upper(col(\"nationality\"))).\n",
    "    withColumn(\"nationality_lower\", lower($\"nationality\")).\n",
    "    withColumn(\"nationality_initcap\", initcap(employeesDF(\"nationality\"))).\n",
    "    withColumn(\"nationality_length\", length(col(\"nationality\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Substring**  \n",
    "Extraction de sous chaine avec la fonction substring. Cette fonction prend en argument la colonne et la position initiale et le nombre de charactère à extraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.substring\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_number\", \"ssn\").\n",
    "    withColumn(\"phone_last4\", substring($\"phone_number\", -4, 4).cast(\"int\")).\n",
    "    withColumn(\"ssn_last4\", substring($\"ssn\", 8, 4).cast(\"int\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting**  \n",
    "Il permet de découper une chaine à partir d'un délimiteur (exemple de delimiteur ' ', ',', ';' etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: List[(Int, String, String, Double, String, String, String)] = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,united KINGDOM,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,AUSTRALIA,+61 987 654 3210,789 12 6118))\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF: org.apache.spark.sql.DataFrame = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_number\", \"ssn\").\n",
    "    withColumn(\"area_code\", split($\"phone_number\", \" \")(1).cast(\"int\")).\n",
    "    withColumn(\"phone_last4\", split($\"phone_number\", \" \")(3).cast(\"int\")).\n",
    "    withColumn(\"ssn_last4\", split($\"ssn\", \" \")(2).cast(\"int\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+---------+----------+---------+\n",
      "|employee_id|    phone_number|        ssn|area_code|phone_last|ssn_last4|\n",
      "+-----------+----------------+-----------+---------+----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|      123|      7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|      234|      8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|      111|      1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|      987|      3210|     6118|\n",
      "+-----------+----------------+-----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"employee_id\", $\"phone_number\", $\"ssn\", \n",
    "           split($\"phone_number\", \" \")(1).cast(\"int\").alias(\"area_code\"),\n",
    "           split($\"phone_number\", \" \")(3).cast(\"int\").alias(\"phone_last\"),\n",
    "           split($\"ssn\", \" \")(2).cast(\"int\").alias(\"ssn_last4\")\n",
    "          ).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`withColumn` permet la creation d'une nouvelle colonne dans le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.concat\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.concat\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\", $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{concat, lit}\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{concat, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\", lit(\", \"), $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding  \n",
    "Permet le remplissage d'une pattern avec une caractère par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{lit, lpad}\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{lit, lpad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l: List[String] = List(X)\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [dummy: string]\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dummy|\n",
      "+----------+\n",
      "|-----Hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit(\"Hello\"), 10, \"-\").alias(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez les fonctions de pad pour convertir chacun des champs en longueur fixe et concaténer. Voici les détails de chacun des champs.\n",
    "\n",
    "* La longueur de employee_id doit être de 5 caractères et doit être complétée par zéro.\n",
    "* La longueur de first_name et last_name doit être de 10 caractères et doit être complétée par - sur le côté droit.\n",
    "* La longueur du salaire doit être de 10 caractères et doit être complétée par zéro.\n",
    "*  La longueur de la nationalité doit être de 15 caractères et doit être complétée par - sur le côté droit.\n",
    "* La longueur du phone_number doit être de 17 caractères et doit être complétée par - sur le côté droit.\n",
    "* La longueur du ssn peut être laissée telle quelle. C'est 11 caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{lpad, rpad, concat}\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{lpad, rpad, concat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empFixedDF: org.apache.spark.sql.DataFrame = [employee: string]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "        lpad($\"employee_id\", 5, \"0\"),\n",
    "        rpad($\"first_name\", 10, \"-\"),\n",
    "        rpad($\"last_name\", 10, \"-\"),\n",
    "        lpad($\"salary\", 10, \"0\"),\n",
    "        rpad($\"nationality\", 15, \"-\"),\n",
    "        rpad($\"phone_number\", 17, \"-\"),\n",
    "        $\"ssn\"\n",
    "    ).alias(\"employee\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            employee|\n",
      "+--------------------+\n",
      "|00001Scott-----Ti...|\n",
      "|00002Henry-----Fo...|\n",
      "|00003Nick------Ju...|\n",
      "|00004Bill------Go...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|employee                                                                      |\n",
      "+------------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming  \n",
    "Il permet de supprimer les espaces vides ou une caractère au début ou à la fin d'une chaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l: List[String] = List(\"   Hello.    \")\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"   Hello.    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [dummy: string]\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, ltrim, rtrim, trim}\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, ltrim, rtrim, trim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+-----+\n",
      "|        dummy|     ltrim|   rtrim| trim|\n",
      "+-------------+----------+--------+-----+\n",
      "|   Hello.    |Hello.    |   Hello|Hello|\n",
      "+-------------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\", ltrim(col(\"dummy\"))).\n",
    "    withColumn(\"rtrim\", rtrim(rtrim(col(\"dummy\")), \".\")).\n",
    "    withColumn(\"trim\", trim(trim(col(\"dummy\")), \".\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date et Time\n",
    "\n",
    "Nous allons avoir un aperçu de la date et de l'heure en utilisant les fonctions disponibles.  \n",
    "Nous pouvons utiliser current_date pour obtenir la date du serveur du jour.  \n",
    "La date sera retournée au format `yyyy-MM-dd`.    \n",
    "Nous pouvons utiliser current_timestamp pour obtenir l'heure actuelle du serveur.  \n",
    "Timestamp permet de renvoyé au format `yyyy-MM-dd HH:mm:ss.SSS`.  \n",
    "Les heures seront par défaut au format 24 heures.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l: List[String] = List(X)\n",
       "df: org.apache.spark.sql.DataFrame = [dummy: string]\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")\n",
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2021-06-30|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date.alias(\"current_date\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_time              |\n",
      "+--------------------------+\n",
      "|2021-06-30 22:17:03.689213|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp.alias(\"current_time\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes: List[(String, String)] = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF: org.apache.spark.sql.DataFrame = [date: string, time: string]\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Date and Time - Arithmetic\n",
    "\n",
    "Effectuons des opérations arithmétique sur la date et l'heure à l'aide des fonctions appropriées.\n",
    "\n",
    "- Ajout de jours à une date ou un timestamp - date_add\n",
    "- Soustraire des jours d'une date ou d'un timestamp - date_sub\n",
    "- Obtenir la différence entre 2 dates ou timestamp - datediff\n",
    "- Obtenir le nombre de mois entre 2 dates ou timestamp - months_between\n",
    "- Ajout de mois à une date ou un timestamp - add_months\n",
    "- Obtenir le jour suivant à partir d'une date donnée - next_day\n",
    "- Toutes ces fonctions sont explicites. Nous pouvons les appliquer sur une date ou un timestamp standard. Toutes les fonctions renvoient une date même lorsqu'elles sont appliquées à un champ de timestamp.\n",
    "\n",
    "\n",
    "### A faire\n",
    "    \n",
    "Effectuons quelques opérations arithmétique sur les dates.\n",
    "\n",
    "- Obtenez d'abord à l'aide sur chaque fonction et comprenez quels sont les arguments qui doivent être passés.\n",
    "- Créez un Dataframe du nom de datetimesDF avec les colonnes date et time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes: List[(String, String)] = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF: org.apache.spark.sql.DataFrame = [date: string, time: string]\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Ajouter 10 jours aux valeurs de la colonne date et time.\n",
    "- Soustraire 10 jours aux valeurs de la colonne date et time.\n",
    "- calculer la différence entre `current_date` et les valeurs de la colonne date ainsi que `current_timestamp` et les valeurs de la colonne time.\n",
    "- calculer le nombre de mois entre l`current_date` et les valeurs de la colonne date ainsi que `current_timestamp`et les valeurs de la colonne time.\n",
    "- Ajoutez 3 mois sur les valeurs de la colonnes de date et time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{date_add, date_sub}\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{date_add, date_sub}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|date      |time                   |date_add_date|date_add_time|date_sub_date|date_sub_time|\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-03-10   |2014-02-18   |2014-02-18   |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10   |2016-03-10   |2016-02-19   |2016-02-19   |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10   |2018-01-10   |2017-10-21   |2017-12-21   |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10   |2019-09-10   |2019-11-20   |2019-08-21   |\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_add_date\", date_add($\"date\", 10)).\n",
    "    withColumn(\"date_add_time\", date_add($\"time\", 10)).\n",
    "    withColumn(\"date_sub_date\", date_sub($\"date\", 10)).\n",
    "    withColumn(\"date_sub_time\", date_sub($\"time\", 10)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{current_date, current_timestamp, datediff}\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp, datediff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n",
      "|date      |time                   |datediff_date|datediff_time|\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2679         |2679         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|1948         |1948         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|1338         |1277         |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|578          |669          |\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"datediff_date\", datediff(current_date, $\"date\")).\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp, $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{months_between, add_months, round}\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{months_between, add_months, round}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|88.0               |88.0               |2014-05-28     |2014-05-28     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|64.0               |64.0               |2016-05-29     |2016-05-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|44.0               |42.0               |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|19.0               |22.0               |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date, $\"date\"), 2)).\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp, $\"time\"), 2)).  \n",
    "    withColumn(\"add_months_date\", add_months($\"date\", 3)).\n",
    "    withColumn(\"add_months_time\", add_months($\"time\", 3)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date et heure - trunc et date_trunc\n",
    "\n",
    "Dans le contexte du Data Warehousing, nous exécutons assez souvent des rapports à ce jour tels que des rapports hebdomadaires, mensuels, annuels, etc.\n",
    "\n",
    "- Nous pouvons utiliser `trunc` ou `date_trunc` pour obtenir la date de début de la semaine, du mois, de l'année en cours, etc. en lui passant date ou timestamp.\n",
    "- Nous pouvons utiliser `trunc` pour obtenir la date de début du mois ou de l'année en lui passant date ou timestamp  - par exemple, trunc(current_date(), \"MM\") donnera le premier du mois en cours.\n",
    "- Nous pouvons utiliser date_trunc pour obtenir la date de début du mois ou de l'année ainsi que l'heure de début du jour ou de l'heure en lui passant l'horodatage.\n",
    "- Obtenir la date de début en fonction du mois - date_trunc(\"MM\", current_timestamp())\n",
    "- Obtenir l'heure de début en fonction du jour - date_trunc(\"DAY\", current_timestamp())\n",
    "\n",
    "### Tâches\n",
    "\n",
    "Exécutons quelques tâches pour comprendre en détail trunc et date_trunc.\n",
    "\n",
    "Créez un Dataframe par nom datetimesDF avec les colonnes date et heure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes: List[(String, String)] = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF: org.apache.spark.sql.DataFrame = [date: string, time: string]\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.trunc\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_trunc\", trunc($\"date\", \"MM\")).\n",
    "    withColumn(\"time_trunc\", trunc($\"time\", \"yyyy\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.date_trunc\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_dt\", date_trunc(\"HOUR\", $\"date\")).\n",
    "    withColumn(\"time_dt\", date_trunc(\"HOUR\", $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date et heure - Extraction d'informations\n",
    "\n",
    "Essayons de comprendre comment extraire des informations à partir de la dates ou de l'heures à l'aide de fonctions.\n",
    "\n",
    "Nous pouvons utiliser date_format pour extraire les informations requises dans un format souhaité à partir de  date ou timestamp.\n",
    "\n",
    "Il existe également des fonctions spécifiques pour extraire l'année, le mois, le jour d'une semaine, un jour d'un mois, un jour d'une année etc.\n",
    "\n",
    "### Tâches\n",
    "\n",
    "Exécutons quelques tâches pour extraire les informations dont nous avons besoin de la date ou le timestamp.\n",
    "\n",
    "Créez un Dataframe par nom datetimesDF avec les colonnes date et heure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes: List[(String, String)] = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF: org.apache.spark.sql.DataFrame = [date: string, time: string]\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
