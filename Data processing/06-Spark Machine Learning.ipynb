{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Machine Learning (MLIb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib est une bibliothèque qui permet de faire du Machile Learning dans Spark. Son objectif est de rendre l'apprentissage automatique pratique évolutif et facile. Cette API fournit un ensemble outils tels que:\n",
    "\n",
    "* **Algorithmes ML**: la plupart des algorithmes d'apprentissage courants tels que la classification, la régression, le clustering et le filtrage collaboratif\n",
    "* **Featurization**: l'extraction de features, la transformation, la réduction de dimension et la selection des features\n",
    "* **Pipelines**: outils de construction, d'évaluation et de tunning des pipelines Machine Learning\n",
    "* **Persistance**: enregistrement et chargement d'algorithmes, de modèles et de pipelines\n",
    "* **Utilitaires**: algèbre linéaire, statistiques, traitement des données, etc.\n",
    "\n",
    "**Plus de details sur Apache Spark MLIb voir la [documentation](https://spark.apache.org/docs/latest/ml-guide.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description des données\n",
    "Airbnb est un service de plateforme communautaire payant de location de vacances, de logements et d'hotels. Dans ce use case, nous allons utiliser les de données de location SF Airbnb de [Inside Airbnb](http://insideairbnb.com/get-the-data.html) pour prédire eventuellement le prix d'un logement d'un client. La figure ci-dessous montre l'activité immobiliere de San Francisco, selon Airbnb.\n",
    " \n",
    " <img src=\"http://insideairbnb.com/images/insideairbnb_graphic_site_1200px.png\" style=\"width:800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons l'ensemble de données SF Airbnb. Les données ci-dessous concernent essentiellement la ville de San Francisco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Dans cette partie nous allons faire l'exploration et le nettoyage des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la listes de features ou attributs du datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,listing_url,scrape_id,last_scraped,name,summary,space,description,experiences_offered,neighborhood_overview,notes,transit,access,interaction,house_rules,thumbnail_url,medium_url,picture_url,xl_picture_url,host_id,host_url,host_name,host_since,host_location,host_about,host_response_time,host_response_rate,host_acceptance_rate,host_is_superhost,host_thumbnail_url,host_picture_url,host_neighbourhood,host_listings_count,host_total_listings_count,host_verifications,host_has_profile_pic,host_identity_verified,street,neighbourhood,neighbourhood_cleansed,neighbourhood_group_cleansed,city,state,zipcode,market,smart_location,country_code,country,latitude,longitude,is_location_exact,property_type,room_type,accommodates,bathrooms,bedrooms,beds,bed_type,amenities,square_feet,price,weekly_price,monthly_price,security_deposit,cleaning_fee,guests_included,extra_people,minimum_nights,maximum_nights,minimum_minimum_nights,maximum_minimum_nights,minimum_maximum_nights,maximum_maximum_nights,minimum_nights_avg_ntm,maximum_nights_avg_ntm,calendar_updated,has_availability,availability_30,availability_60,availability_90,availability_365,calendar_last_scraped,number_of_reviews,number_of_reviews_ltm,first_review,last_review,review_scores_rating,review_scores_accuracy,review_scores_cleanliness,review_scores_checkin,review_scores_communication,review_scores_location,review_scores_value,requires_license,license,jurisdiction_names,instant_bookable,is_business_travel_ready,cancellation_policy,require_guest_profile_picture,require_guest_phone_verification,calculated_host_listings_count,calculated_host_listings_count_entire_homes,calculated_host_listings_count_private_rooms,calculated_host_listings_count_shared_rooms,reviews_per_month\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -1 \"datasets/sf-airbnb/sf-airbnb.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données dans un DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.115.226:4043\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1659197725312)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "filePath: String = datasets/sf-airbnb/sf-airbnb.csv\n",
       "rawDF: org.apache.spark.sql.DataFrame = [id: int, listing_url: string ... 104 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filePath = \"datasets/sf-airbnb/sf-airbnb.csv\"\n",
    "\n",
    "val rawDF = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"escape\", \"\\\"\")\n",
    "  .csv(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- last_scraped: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- space: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- experiences_offered: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- transit: string (nullable = true)\n",
      " |-- access: string (nullable = true)\n",
      " |-- interaction: string (nullable = true)\n",
      " |-- house_rules: string (nullable = true)\n",
      " |-- thumbnail_url: string (nullable = true)\n",
      " |-- medium_url: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- xl_picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- smart_location: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- is_location_exact: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- weekly_price: string (nullable = true)\n",
      " |-- monthly_price: string (nullable = true)\n",
      " |-- security_deposit: string (nullable = true)\n",
      " |-- cleaning_fee: string (nullable = true)\n",
      " |-- guests_included: integer (nullable = true)\n",
      " |-- extra_people: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: string (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- first_review: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- review_scores_rating: integer (nullable = true)\n",
      " |-- review_scores_accuracy: integer (nullable = true)\n",
      " |-- review_scores_cleanliness: integer (nullable = true)\n",
      " |-- review_scores_checkin: integer (nullable = true)\n",
      " |-- review_scores_communication: integer (nullable = true)\n",
      " |-- review_scores_location: integer (nullable = true)\n",
      " |-- review_scores_value: integer (nullable = true)\n",
      " |-- requires_license: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- jurisdiction_names: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- is_business_travel_ready: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- require_guest_profile_picture: string (nullable = true)\n",
      " |-- require_guest_phone_verification: string (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[String] = Array(id, listing_url, scrape_id, last_scraped, name, summary, space, description, experiences_offered, neighborhood_overview, notes, transit, access, interaction, house_rules, thumbnail_url, medium_url, picture_url, xl_picture_url, host_id, host_url, host_name, host_since, host_location, host_about, host_response_time, host_response_rate, host_acceptance_rate, host_is_superhost, host_thumbnail_url, host_picture_url, host_neighbourhood, host_listings_count, host_total_listings_count, host_verifications, host_has_profile_pic, host_identity_verified, street, neighbourhood, neighbourhood_cleansed, neighbourhood_group_cleansed, city, state, zipcode, market, smart_location, country_code, country, latitude, longitude, is_location_exact, property_type, room_type, accommod...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Int = 106\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDF.columns.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " path file:/home/dmboup/Desktop/apache_spark_data_processing/apache_spark_data_processing/datasets/sf-airbnb/sf_airbnb_prep.csv already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/home/dmboup/Desktop/apache_spark_data_processing/apache_spark_data_processing/datasets/sf-airbnb/sf_airbnb_prep.csv already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:121)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)",
      "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "val df = rawDF.select(\"id\",\"name\",\"host_id\",\"host_name\"\n",
    "                      ,\"neighbourhood\",\n",
    "                      \"latitude\",\"longitude\",\"room_type\",\"price\",\n",
    "                      \"minimum_nights\",\"number_of_reviews\",\"last_review\",\n",
    "                      \"reviews_per_month\",\"calculated_host_listings_count\",\"availability_365\")\n",
    "df.write.csv(\"datasets/sf-airbnb/sf_airbnb_prep.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Dans ce notebook nous avons sauter la partie feature extration et feature selection.  \n",
    "Pour simplifier la tache, nous avons choisi de garder que les colonnes utiles du datasets apres l'étape de feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val baseDF = rawDF.select(\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"host_total_listings_count\",\n",
    "  \"neighbourhood_cleansed\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"property_type\",\n",
    "  \"room_type\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\",\n",
    "  \"price\")\n",
    "\n",
    "baseDF.cache().count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le nombre colonnes élevé, l'affichage devient illisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+\n",
      "|host_is_superhost|cancellation_policy|instant_bookable|host_total_listings_count|neighbourhood_cleansed|latitude| longitude|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|  price|\n",
      "+-----------------+-------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+\n",
      "|                t|           moderate|               t|                        1|      Western Addition|37.76931|-122.43386|    Apartment|Entire home/apt|           3|      1.0|       1|   2|Real Bed|             1|              180|                  97|                    10|                       10|                   10|                         10|                    10|                 10|$170.00|\n",
      "+-----------------+-------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi quelques colonnes du datasets pour avoir une bonne rendue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-------+\n",
      "|host_is_superhost|instant_bookable|latitude| longitude|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|  price|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-------+\n",
      "|                t|               t|37.76931|-122.43386|           3|      1.0|       1|   2|Real Bed|             1|              180|$170.00|\n",
      "|                f|               f|37.74511|-122.42102|           5|      1.0|       2|   3|Real Bed|            30|              111|$235.00|\n",
      "|                f|               f|37.76669| -122.4525|           2|      4.0|       1|   1|Real Bed|            32|               17| $65.00|\n",
      "|                f|               f|37.76487|-122.45183|           2|      4.0|       1|   1|Real Bed|            32|                8| $65.00|\n",
      "|                f|               f|37.77525|-122.43637|           5|      1.5|       2|   2|Real Bed|             7|               27|$785.00|\n",
      "|                f|               f|37.78471|-122.44555|           6|      1.0|       2|   3|Real Bed|             2|               31|$255.00|\n",
      "|                t|               t|37.75919|-122.42237|           3|      1.0|       1|   2|Real Bed|             1|              647|$139.00|\n",
      "|                f|               f|37.76259|-122.40543|           2|      1.0|       1|   1|Real Bed|             1|              453|$135.00|\n",
      "|                t|               f|37.75874|-122.41327|           6|      1.0|       2|   3|Real Bed|             3|              320|$265.00|\n",
      "|                f|               f|37.77187|-122.43859|           3|      1.0|       3|   3|Real Bed|            30|               37|$177.00|\n",
      "|                f|               f|37.77355|-122.42436|           5|      2.0|       3|   3|Real Bed|            30|               14|$194.00|\n",
      "|                f|               f|37.78574|-122.40798|           2|      1.5|       1|   1|Real Bed|            30|               19|$139.00|\n",
      "|                f|               f|37.77019|-122.44594|           2|      4.0|       1|   2|Real Bed|            32|                6| $85.00|\n",
      "|                f|               f|37.76894|-122.44778|           2|      3.0|       1|   2|Real Bed|            32|                5| $85.00|\n",
      "|                t|               t|37.76075|-122.43032|           1|      1.0|       1|   1|Real Bed|             3|              390| $79.00|\n",
      "|                t|               f|37.76203|-122.45455|           3|      1.0|       2|   3|Real Bed|            30|               16|$136.00|\n",
      "|                f|               f|37.75491|-122.42246|           3|      1.0|       1|   2|Real Bed|             3|              103|$215.00|\n",
      "|                f|               f|37.78647|-122.39072|           4|      2.0|       2|   2|Real Bed|            90|               14|$450.00|\n",
      "|                t|               f|37.74888|-122.42982|           3|      1.0|       0|   1|Real Bed|            30|               61|$107.00|\n",
      "|                t|               f|37.77252|-122.43216|           2|      1.0|       1|   1|Real Bed|             2|              363|$110.00|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseDF.select(\n",
    "  \"host_is_superhost\",\n",
    "  \"instant_bookable\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"price\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le schéma ci-dessus. Vous remarquerez que le champ `price` a été sélectionné sous forme de chaîne. Dans cette tache, nous avons besoin qu'il soit champ numérique (type double)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.translate\n",
       "fixedPriceDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.translate\n",
    "\n",
    "val fixedPriceDF = baseDF.withColumn(\"price\", translate($\"price\", \"$\", \"\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "|host_is_superhost|instant_bookable|latitude| longitude|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|price|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "|                t|               t|37.76931|-122.43386|           3|      1.0|       1|   2|Real Bed|             1|              180|170.0|\n",
      "|                f|               f|37.74511|-122.42102|           5|      1.0|       2|   3|Real Bed|            30|              111|235.0|\n",
      "|                f|               f|37.76669| -122.4525|           2|      4.0|       1|   1|Real Bed|            32|               17| 65.0|\n",
      "|                f|               f|37.76487|-122.45183|           2|      4.0|       1|   1|Real Bed|            32|                8| 65.0|\n",
      "|                f|               f|37.77525|-122.43637|           5|      1.5|       2|   2|Real Bed|             7|               27|785.0|\n",
      "|                f|               f|37.78471|-122.44555|           6|      1.0|       2|   3|Real Bed|             2|               31|255.0|\n",
      "|                t|               t|37.75919|-122.42237|           3|      1.0|       1|   2|Real Bed|             1|              647|139.0|\n",
      "|                f|               f|37.76259|-122.40543|           2|      1.0|       1|   1|Real Bed|             1|              453|135.0|\n",
      "|                t|               f|37.75874|-122.41327|           6|      1.0|       2|   3|Real Bed|             3|              320|265.0|\n",
      "|                f|               f|37.77187|-122.43859|           3|      1.0|       3|   3|Real Bed|            30|               37|177.0|\n",
      "|                f|               f|37.77355|-122.42436|           5|      2.0|       3|   3|Real Bed|            30|               14|194.0|\n",
      "|                f|               f|37.78574|-122.40798|           2|      1.5|       1|   1|Real Bed|            30|               19|139.0|\n",
      "|                f|               f|37.77019|-122.44594|           2|      4.0|       1|   2|Real Bed|            32|                6| 85.0|\n",
      "|                f|               f|37.76894|-122.44778|           2|      3.0|       1|   2|Real Bed|            32|                5| 85.0|\n",
      "|                t|               t|37.76075|-122.43032|           1|      1.0|       1|   1|Real Bed|             3|              390| 79.0|\n",
      "|                t|               f|37.76203|-122.45455|           3|      1.0|       2|   3|Real Bed|            30|               16|136.0|\n",
      "|                f|               f|37.75491|-122.42246|           3|      1.0|       1|   2|Real Bed|             3|              103|215.0|\n",
      "|                f|               f|37.78647|-122.39072|           4|      2.0|       2|   2|Real Bed|            90|               14|450.0|\n",
      "|                t|               f|37.74888|-122.42982|           3|      1.0|       0|   1|Real Bed|            30|               61|107.0|\n",
      "|                t|               f|37.77252|-122.43216|           2|      1.0|       1|   1|Real Bed|             2|              363|110.0|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixedPriceDF.select(\n",
    "  \"host_is_superhost\",\n",
    "  \"instant_bookable\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"price\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelques statistiques sur les données avec Describe & Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "desc: org.apache.spark.sql.DataFrame = [summary: string, host_is_superhost: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val desc = fixedPriceDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|host_is_superhost|            latitude|           longitude|              beds|    minimum_nights|number_of_reviews|             price|\n",
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             7151|                7151|                7151|              7144|              7151|             7151|              7059|\n",
      "|   mean|             null|   37.76580945042649| -122.43052552230478|1.7648376259798433|14000.302335337716|43.52915676129213|191.33914152146195|\n",
      "| stddev|             null|0.022527191846014046|0.026791775802673057| 1.176852628831775|1182541.9078980184|72.51922886627213|141.76871042769946|\n",
      "|    min|                f|            37.70743|          -122.51306|                 0|                 1|                0|               0.0|\n",
      "|    max|                t|            37.81031|          -122.36979|                14|         100000000|              677|             999.0|\n",
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc.select(\n",
    "\"summary\",\n",
    "\"host_is_superhost\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"beds\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.DataFrame = [summary: string, host_is_superhost: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stats = fixedPriceDF.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|host_is_superhost|            latitude|           longitude|              beds|    minimum_nights|number_of_reviews|             price|\n",
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             7151|                7151|                7151|              7144|              7151|             7151|              7059|\n",
      "|   mean|             null|   37.76580945042649| -122.43052552230478|1.7648376259798433|14000.302335337716|43.52915676129213|191.33914152146195|\n",
      "| stddev|             null|0.022527191846014046|0.026791775802673057| 1.176852628831775|1182541.9078980184|72.51922886627213|141.76871042769946|\n",
      "|    min|                f|            37.70743|          -122.51306|                 0|                 1|                0|               0.0|\n",
      "|    25%|             null|            37.75111|          -122.44295|                 1|                 2|                1|             100.0|\n",
      "|    50%|             null|            37.76755|          -122.42547|                 1|                 4|               11|             150.0|\n",
      "|    75%|             null|            37.81031|          -122.36979|                14|         100000000|              677|             999.0|\n",
      "|    max|                t|            37.81031|          -122.36979|                14|         100000000|              677|             999.0|\n",
      "+-------+-----------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats.select(\n",
    "\"summary\",\n",
    "\"host_is_superhost\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"beds\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation des valeurs nulles\n",
    "\n",
    "Il existe de nombreuses façons de gérer les valeurs nulles. Parfois, **null** peut, en fait, être un indicateur clé de ce que vous essayez de prédire (par exemple, si vous ne remplissez pas certaines parties d'un formulaire, la probabilité qu'il soit approuvé diminue).\n",
    " \n",
    "  Quelques façons de gérer les valeurs nulles:\n",
    "  * Supprimez tous les enregistrements contenant des valeurs nulles\n",
    "  * Numérique:\n",
    "    * Remplacez-les par moyenne/médiane/zéro/etc.\n",
    "  * Catégorique:\n",
    "    * Remplacez-les par le mode\n",
    "    * Créez une catégorie spéciale pour null\n",
    "  * Utilisez d'autres techniques pour imputer les valeurs manquantes\n",
    "   \n",
    "**Si vous appliquez TOUTES les techniques d'imputation pour les attributs de type catégoriques/numériques, vous DEVEZ inclure un champ supplémentaire spécifiant que ce champ a été imputé (pensez à la raison pour laquelle cela est nécessaire)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a quelques valeurs nulles dans l'attribut catégorique `host_is_superhost`. Débarrassons-nous des lignes où l'une de ces colonnes est nulle.\n",
    " \n",
    "L'option d'imputation de SparkML ne prend pas en charge l'imputation pour les attributs catégoriques, c'est donc l'approche la plus simple pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noNullsDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val noNullsDF = fixedPriceDF.na.drop(cols = Seq(\"host_is_superhost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting des attributs de Integer vers le type Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.sql.types.IntegerType\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes convertis de Integer vers Double:\n",
      " - host_total_listings_count\n",
      " - accommodates\n",
      " - bedrooms\n",
      " - beds\n",
      " - minimum_nights\n",
      " - number_of_reviews\n",
      " - review_scores_rating\n",
      " - review_scores_accuracy\n",
      " - review_scores_cleanliness\n",
      " - review_scores_checkin\n",
      " - review_scores_communication\n",
      " - review_scores_location\n",
      " - review_scores_value \n",
      "\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "integerColumns: Array[String] = Array(host_total_listings_count, accommodates, bedrooms, beds, minimum_nights, number_of_reviews, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value)\n",
       "doublesDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 22 more fields]\n",
       "columns: String =\n",
       "host_total_listings_count\n",
       " - accommodates\n",
       " - bedrooms\n",
       " - beds\n",
       " - minimum_nights\n",
       " - number_of_reviews\n",
       " - review_scores_rating\n",
       " - review_scores_accuracy\n",
       " - review_scores_cleanliness\n",
       " - review_scores_checkin\n",
       " - review_scores_communication\n",
       " - review_scores_location\n",
       " - review_scores_value\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val integerColumns = for (x <- baseDF.schema.fields if (x.dataType == IntegerType)) yield x.name  \n",
    "var doublesDF = noNullsDF\n",
    "\n",
    "for (c <- integerColumns)\n",
    "  doublesDF = doublesDF.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "val columns = integerColumns.mkString(\"\\n - \")\n",
    "println(s\"Colonnes convertis de Integer vers Double:\\n - $columns \\n\")\n",
    "println(\"*-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ajouter une variable factive si nous allons imputer une valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.when\n",
       "imputeCols: Array[String] = Array(bedrooms, bathrooms, beds, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.when\n",
    "\n",
    "val imputeCols = Array(\n",
    "  \"bedrooms\",\n",
    "  \"bathrooms\",\n",
    "  \"beds\", \n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\"\n",
    ")\n",
    "\n",
    "for (c <- imputeCols)\n",
    "  doublesDF = doublesDF.withColumn(c + \"_na\", when(col(c).isNull, 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+------------+---------+--------+----+-------+--------+--------------+-----------------+--------------------+----------------------+-----+\n",
      "|host_is_superhost|instant_bookable|accommodates|bathrooms|bedrooms|beds|beds_na|bed_type|minimum_nights|number_of_reviews|review_scores_rating|review_scores_accuracy|price|\n",
      "+-----------------+----------------+------------+---------+--------+----+-------+--------+--------------+-----------------+--------------------+----------------------+-----+\n",
      "|                t|               t|         3.0|      1.0|     1.0| 2.0|    0.0|Real Bed|           1.0|            180.0|                97.0|                  10.0|170.0|\n",
      "|                f|               f|         5.0|      1.0|     2.0| 3.0|    0.0|Real Bed|          30.0|            111.0|                98.0|                  10.0|235.0|\n",
      "|                f|               f|         2.0|      4.0|     1.0| 1.0|    0.0|Real Bed|          32.0|             17.0|                85.0|                   8.0| 65.0|\n",
      "|                f|               f|         2.0|      4.0|     1.0| 1.0|    0.0|Real Bed|          32.0|              8.0|                93.0|                   9.0| 65.0|\n",
      "|                f|               f|         5.0|      1.5|     2.0| 2.0|    0.0|Real Bed|           7.0|             27.0|                97.0|                  10.0|785.0|\n",
      "|                f|               f|         6.0|      1.0|     2.0| 3.0|    0.0|Real Bed|           2.0|             31.0|                90.0|                   9.0|255.0|\n",
      "|                t|               t|         3.0|      1.0|     1.0| 2.0|    0.0|Real Bed|           1.0|            647.0|                98.0|                  10.0|139.0|\n",
      "|                f|               f|         2.0|      1.0|     1.0| 1.0|    0.0|Real Bed|           1.0|            453.0|                94.0|                  10.0|135.0|\n",
      "|                t|               f|         6.0|      1.0|     2.0| 3.0|    0.0|Real Bed|           3.0|            320.0|                96.0|                  10.0|265.0|\n",
      "|                f|               f|         3.0|      1.0|     3.0| 3.0|    0.0|Real Bed|          30.0|             37.0|                89.0|                   9.0|177.0|\n",
      "|                f|               f|         5.0|      2.0|     3.0| 3.0|    0.0|Real Bed|          30.0|             14.0|                91.0|                  10.0|194.0|\n",
      "|                f|               f|         2.0|      1.5|     1.0| 1.0|    0.0|Real Bed|          30.0|             19.0|                92.0|                  10.0|139.0|\n",
      "|                f|               f|         2.0|      4.0|     1.0| 2.0|    0.0|Real Bed|          32.0|              6.0|                80.0|                   9.0| 85.0|\n",
      "|                f|               f|         2.0|      3.0|     1.0| 2.0|    0.0|Real Bed|          32.0|              5.0|                64.0|                   6.0| 85.0|\n",
      "|                t|               t|         1.0|      1.0|     1.0| 1.0|    0.0|Real Bed|           3.0|            390.0|                98.0|                  10.0| 79.0|\n",
      "|                t|               f|         3.0|      1.0|     2.0| 3.0|    0.0|Real Bed|          30.0|             16.0|                95.0|                   9.0|136.0|\n",
      "|                f|               f|         3.0|      1.0|     1.0| 2.0|    0.0|Real Bed|           3.0|            103.0|                97.0|                  10.0|215.0|\n",
      "|                f|               f|         4.0|      2.0|     2.0| 2.0|    0.0|Real Bed|          90.0|             14.0|                96.0|                  10.0|450.0|\n",
      "|                t|               f|         3.0|      1.0|     0.0| 1.0|    0.0|Real Bed|          30.0|             61.0|                96.0|                  10.0|107.0|\n",
      "|                t|               f|         2.0|      1.0|     1.0| 1.0|    0.0|Real Bed|           2.0|            363.0|                97.0|                  10.0|110.0|\n",
      "+-----------------+----------------+------------+---------+--------+----+-------+--------+--------------+-----------------+--------------------+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doublesDF.select(\n",
    "\"host_is_superhost\",\n",
    "  \"instant_bookable\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"beds_na\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"price\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Imputer\n",
       "imputer: org.apache.spark.ml.feature.Imputer = imputer_a805529d38e9\n",
       "imputedDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setStrategy(\"median\")\n",
    "  .setInputCols(imputeCols)\n",
    "  .setOutputCols(imputeCols)\n",
    "\n",
    "val imputedDF = imputer.fit(doublesDF).transform(doublesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut jeter un coup d'œil sur les valeurs *min* et *max* de la colonne `price`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             price|\n",
      "+-------+------------------+\n",
      "|  count|              7059|\n",
      "|   mean|191.33914152146195|\n",
      "| stddev|141.76871042769946|\n",
      "|    min|               0.0|\n",
      "|    max|             999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputedDF.select(\"price\").describe().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe des annonces qui sont très chères. Mais c'est le travail du Data Scientist de décider quoi en faire. Nous pouvons certainement filtrer les Airbnbs \"gratuits\".\n",
    " \n",
    "Voyons d'abord, combien d'annonces  dont leur *prix* est nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 1\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputedDF.filter($\"price\" === 0).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maintenant, nous allons seulement garder les lignes dont les prix sont strictement positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "posPricesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val posPricesDF = imputedDF.filter($\"price\" > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons le min max de la colonne *minimum_nights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    minimum_nights|\n",
      "+-------+------------------+\n",
      "|  count|              7058|\n",
      "|   mean| 14184.45820345707|\n",
      "| stddev|1190307.3162499827|\n",
      "|    min|               1.0|\n",
      "|    max|             1.0E8|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posPricesDF.select(\"minimum_nights\").describe().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|minimum_nights|count|\n",
      "+--------------+-----+\n",
      "|          30.0| 2744|\n",
      "|           2.0| 1433|\n",
      "|           1.0| 1232|\n",
      "|           3.0|  808|\n",
      "|           4.0|  258|\n",
      "|           5.0|  170|\n",
      "|          31.0|  132|\n",
      "|           7.0|   72|\n",
      "|           6.0|   31|\n",
      "|          32.0|   31|\n",
      "|          60.0|   31|\n",
      "|         180.0|   28|\n",
      "|          90.0|   27|\n",
      "|          45.0|    7|\n",
      "|         120.0|    6|\n",
      "|         365.0|    6|\n",
      "|          14.0|    4|\n",
      "|          10.0|    3|\n",
      "|          40.0|    3|\n",
      "|          28.0|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posPricesDF\n",
    "  .groupBy(\"minimum_nights\").count()\n",
    "  .orderBy($\"count\".desc, $\"minimum_nights\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un séjour minimum d'un an semble être une limite raisonnable ici. Filtrons les enregistrements où le *minimum_nights* est supérieur à 365 jours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanDF = posPricesDF.filter($\"minimum_nights\" <= 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "|host_is_superhost|instant_bookable|latitude| longitude|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|price|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "|                t|               t|37.76931|-122.43386|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            180.0|170.0|\n",
      "|                f|               f|37.74511|-122.42102|         5.0|      1.0|     2.0| 3.0|Real Bed|          30.0|            111.0|235.0|\n",
      "|                f|               f|37.76669| -122.4525|         2.0|      4.0|     1.0| 1.0|Real Bed|          32.0|             17.0| 65.0|\n",
      "|                f|               f|37.76487|-122.45183|         2.0|      4.0|     1.0| 1.0|Real Bed|          32.0|              8.0| 65.0|\n",
      "|                f|               f|37.77525|-122.43637|         5.0|      1.5|     2.0| 2.0|Real Bed|           7.0|             27.0|785.0|\n",
      "|                f|               f|37.78471|-122.44555|         6.0|      1.0|     2.0| 3.0|Real Bed|           2.0|             31.0|255.0|\n",
      "|                t|               t|37.75919|-122.42237|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            647.0|139.0|\n",
      "|                f|               f|37.76259|-122.40543|         2.0|      1.0|     1.0| 1.0|Real Bed|           1.0|            453.0|135.0|\n",
      "|                t|               f|37.75874|-122.41327|         6.0|      1.0|     2.0| 3.0|Real Bed|           3.0|            320.0|265.0|\n",
      "|                f|               f|37.77187|-122.43859|         3.0|      1.0|     3.0| 3.0|Real Bed|          30.0|             37.0|177.0|\n",
      "|                f|               f|37.77355|-122.42436|         5.0|      2.0|     3.0| 3.0|Real Bed|          30.0|             14.0|194.0|\n",
      "|                f|               f|37.78574|-122.40798|         2.0|      1.5|     1.0| 1.0|Real Bed|          30.0|             19.0|139.0|\n",
      "|                f|               f|37.77019|-122.44594|         2.0|      4.0|     1.0| 2.0|Real Bed|          32.0|              6.0| 85.0|\n",
      "|                f|               f|37.76894|-122.44778|         2.0|      3.0|     1.0| 2.0|Real Bed|          32.0|              5.0| 85.0|\n",
      "|                t|               t|37.76075|-122.43032|         1.0|      1.0|     1.0| 1.0|Real Bed|           3.0|            390.0| 79.0|\n",
      "|                t|               f|37.76203|-122.45455|         3.0|      1.0|     2.0| 3.0|Real Bed|          30.0|             16.0|136.0|\n",
      "|                f|               f|37.75491|-122.42246|         3.0|      1.0|     1.0| 2.0|Real Bed|           3.0|            103.0|215.0|\n",
      "|                f|               f|37.78647|-122.39072|         4.0|      2.0|     2.0| 2.0|Real Bed|          90.0|             14.0|450.0|\n",
      "|                t|               f|37.74888|-122.42982|         3.0|      1.0|     0.0| 1.0|Real Bed|          30.0|             61.0|107.0|\n",
      "|                t|               f|37.77252|-122.43216|         2.0|      1.0|     1.0| 1.0|Real Bed|           2.0|            363.0|110.0|\n",
      "+-----------------+----------------+--------+----------+------------+---------+--------+----+--------+--------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDF.select(\n",
    "  \"host_is_superhost\",\n",
    "  \"instant_bookable\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"price\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, nos données sont maintenant nettoyées. Maintenant, nous pouvons sauvegarder ce DataFrame dans un fichier afin de pouvoir l'utiliser pour faire des modèle ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputPath: String = datasets/sf-airbnb/sf-airbnb-clean.parquet\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputPath = \"datasets/sf-airbnb/sf-airbnb-clean.parquet\"\n",
    "\n",
    "cleanDF.write.mode(\"overwrite\").parquet(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning modèles\n",
    "Nous rappelons, notre objectif est de developper un modèle ML de prédire les prix de location à partir des données de Airbnb à San Francisco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons utiliser les données que nous avons nettoyé et souvegardé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filePath: String = datasets/sf-airbnb/sf-airbnb-clean.parquet\n",
       "airbnbDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filePath = \"datasets/sf-airbnb/sf-airbnb-clean.parquet\"\n",
    "val airbnbDF = spark.read.parquet(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "|host_is_superhost| cancellation_policy|instant_bookable|host_total_listings_count|neighbourhood_cleansed|latitude| longitude|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|price|bedrooms_na|bathrooms_na|beds_na|review_scores_rating_na|review_scores_accuracy_na|review_scores_cleanliness_na|review_scores_checkin_na|review_scores_communication_na|review_scores_location_na|review_scores_value_na|\n",
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "|                t|            moderate|               t|                      1.0|      Western Addition|37.76931|-122.43386|    Apartment|Entire home/apt|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            180.0|                97.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|               10.0|170.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
      "|                f|strict_14_with_gr...|               f|                      2.0|        Bernal Heights|37.74511|-122.42102|    Apartment|Entire home/apt|         5.0|      1.0|     2.0| 3.0|Real Bed|          30.0|            111.0|                98.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|                9.0|235.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on construit un modèle Machine Learning, pourquoi on utilise pas les données de test ?\n",
    " \n",
    "Nous allons prendre 80% des données pour le train et 20% pour le test. Nous utiliserons la méthode `randomSplit` [Python](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Il y a 5708 lignes dans le  training set, et 1347 dans le test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n",
       "testDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainDF, testDF) = airbnbDF.randomSplit(Array(.8, .2), seed=42)\n",
    "println(f\" Il y a ${trainDF.cache().count()} lignes dans le  training set, et ${testDF.cache().count()} dans le test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainRepartitionDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n",
       "testRepartitionDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host_is_superhost: string, cancellation_policy: string ... 32 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainRepartitionDF, testRepartitionDF) = airbnbDF\n",
    "  .repartition(24)\n",
    "  .randomSplit(Array(.8, .2), seed=42)\n",
    "\n",
    "println(trainRepartitionDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *spliting* de train/test 80/20 n'est pas exacte, il est juste «approximatif». Lorsque le partitionnement des données change, la taille des donnée est différent dans le train/test ainsi que les data ponts différents.\n",
    " \n",
    "Nous recommandation est de partitionner vos données une fois, puis de les écrire dans un dossier train/test afin d'éviter ces problèmes de reproductibilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons construire un modèle de régression linéaire très simple prédisant le **\"prix\"** juste compte tenu du nombre de **\"chambres\"**.\n",
    " \n",
    "**Question**: Quelles sont hypothèses du modèle de régression linéaire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             price|          bedrooms|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              5708|              5708|\n",
      "|   mean|191.64453398738613| 1.326208829712684|\n",
      "| stddev|142.23860298327358|0.9083263732042874|\n",
      "|    min|              10.0|               0.0|\n",
      "|    25%|             100.0|               1.0|\n",
      "|    50%|             150.0|               1.0|\n",
      "|    75%|             230.0|               2.0|\n",
      "|    max|             999.0|              14.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"price\", \"bedrooms\").summary().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des valeurs aberrantes dans les données pour le prix (Par exemple 10 000$ la nuité ??). Faite attention à cela, lorsque vous construisez vos modèles :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce premier modèle, nous allons  prédire le prix du loyer en fonctions du nombre de chambres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembler\n",
    "La régression linéaire prend une colonne de type Vector comme entrée.\n",
    " \n",
    " Nous pouvons facilement obtenir les valeurs de la colonne `chambres` dans un seul vecteur en utilisant `VectorAssembler` [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark. ml.feature.VectorAssembler)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler). VectorAssembler est un exemple de **transformer**. Les *transformers* prennent un DataFrame et renvoient un nouveau DataFrame avec une ou plusieurs colonnes qui lui sont ajoutées. Ils n'apprennent pas de vos données, mais appliquent des transformations basées sur des règles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_4e00ae925b87, handleInvalid=error, numInputCols=1\n",
       "vecTrainDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 33 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val vecAssembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"bedrooms\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val vecTrainDF = vecAssembler.transform(trainDF)\n",
    "\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Linéaire\n",
    "\n",
    "Maintenant que nous avons préparé les données, nous pouvons utiliser l'estimateur `LinearRegression` pour construire notre premier modèle [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark .ml.regression.LinearRegression)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.regression.LinearRegression). Les estimateurs acceptent un DataFrame comme entrée et retournent un modèle. Ils ont une méthode `.fit ()` pour faire le fitting ou l'entrainement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_9fe2eb78a95b\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_9fe2eb78a95b, numFeatures=1\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "val lr = new LinearRegression()\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"price\")\n",
    "\n",
    "val lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La droite de regression price= a* bedrooms +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La droite de Regression price = 90.94*bedrooms + 71.04\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m: Double = 90.93678677027269\n",
       "b: Double = 71.0433644269509\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val m = lrModel.coefficients(0)\n",
    "val b = lrModel.intercept\n",
    "\n",
    "println(f\"La droite de Regression price = $m%1.2f*bedrooms + $b%1.2f\")\n",
    "println(\"*-\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_32375b7f4dee\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_32375b7f4dee\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(vecAssembler, lr))\n",
    "val pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------------------+\n",
      "|bedrooms|features|price|        prediction|\n",
      "+--------+--------+-----+------------------+\n",
      "|     1.0|   [1.0]| 85.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]| 45.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]| 70.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]|128.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]|159.0| 161.9801511972236|\n",
      "|     2.0|   [2.0]|250.0|252.91693796749627|\n",
      "|     1.0|   [1.0]| 99.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]| 95.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]|100.0| 161.9801511972236|\n",
      "|     1.0|   [1.0]|270.0| 161.9801511972236|\n",
      "+--------+--------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons le «prix» moyen sur les données d'entraînement, et utilisons-le comme colonne de prédiction sur les données de test, puis évaluons le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{avg, lit}\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{avg, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgPrice: Double = 191.64453398738613\n",
       "predDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 33 more fields]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val avgPrice = trainDF.select(avg(\"price\")).first().getDouble(0)\n",
    "\n",
    "val predDF = testDF.withColumn(\"avgPrediction\", lit(avgPrice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Root Mean Square Error* (RMSE) ou Erreur quadratique moyenne définit ci-dessous nous permet d'évaluer le modèle.  \n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}(y_i- \\hat{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "regressionMeanEvaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_17e8f40d60fd, metricName=rmse, throughOrigin=false\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val regressionMeanEvaluator = new RegressionEvaluator()\n",
    "  .setPredictionCol(\"avgPrediction\")\n",
    "  .setLabelCol(\"price\")\n",
    "  .setMetricName(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE pour la prediction du prix moyen de loyer: 139.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rmse: Double = 139.81513140129226\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rmse = regressionMeanEvaluator.evaluate(predDF)\n",
    "println (f\"The RMSE pour la prediction du prix moyen de loyer: $rmse%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle 2.\n",
    "Pour prèdire le prix du loyer nous allons ajouter d'autres features supplementaires, en faisant OneHoteEncoding pour les attributs categoriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous allons faire du OneHotEncode (OHE) des variables catégoriques. La première approche que nous allons utiliser combinera `StringIndexer`, `OneHotEncoder` et `VectorAssembler`.\n",
    "* Nous devons d'abord utiliser `StringIndexer` pour mapper la colonne en string des labels à une colonne d'index de labels [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.StringIndexer).\n",
    "* Ensuite, nous pouvons appliquer le `OneHotEncoder` à la sortie du `StringIndexer` [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder )/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.OneHotEncoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
       "categoricalCols: Array[String] = Array(host_is_superhost, cancellation_policy, instant_bookable, neighbourhood_cleansed, property_type, room_type, bed_type)\n",
       "indexOutputCols: Array[String] = Array(host_is_superhostIndex, cancellation_policyIndex, instant_bookableIndex, neighbourhood_cleansedIndex, property_typeIndex, room_typeIndex, bed_typeIndex)\n",
       "oheOutputCols: Array[String] = Array(host_is_superhostOHE, cancellation_policyOHE, instant_bookableOHE, neighbourhood_cleansedOHE, property_typeOHE, room_typeOHE, bed_typeOHE)\n",
       "stringIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_8adf69b2bb91\n",
       "oheEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_1aa4a8e7119c\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
    "\n",
    "val categoricalCols = trainDF.dtypes.filter(_._2 == \"StringType\").map(_._1)\n",
    "val indexOutputCols = categoricalCols.map(_ + \"Index\")\n",
    "val oheOutputCols = categoricalCols.map(_ + \"OHE\")\n",
    "\n",
    "val stringIndexer = new StringIndexer()\n",
    "  .setInputCols(categoricalCols)\n",
    "  .setOutputCols(indexOutputCols)\n",
    "  .setHandleInvalid(\"skip\")\n",
    "\n",
    "val oheEncoder = new OneHotEncoder()\n",
    "  .setInputCols(indexOutputCols)\n",
    "  .setOutputCols(oheOutputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais combiner les attributs catégoriques OHE avec les attributs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "numericCols: Array[String] = Array(host_total_listings_count, latitude, longitude, accommodates, bathrooms, bedrooms, beds, minimum_nights, number_of_reviews, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value, bedrooms_na, bathrooms_na, beds_na, review_scores_rating_na, review_scores_accuracy_na, review_scores_cleanliness_na, review_scores_checkin_na, review_scores_communication_na, review_scores_location_na, review_scores_value_na)\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val numericCols = trainDF.dtypes.filter{ case (field, dataType) => \n",
    "                      dataType == \"DoubleType\" && field != \"price\"}.map(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assemblerInputs: Array[String] = Array(host_is_superhostOHE, cancellation_policyOHE, instant_bookableOHE, neighbourhood_cleansedOHE, property_typeOHE, room_typeOHE, bed_typeOHE, host_total_listings_count, latitude, longitude, accommodates, bathrooms, bedrooms, beds, minimum_nights, number_of_reviews, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value, bedrooms_na, bathrooms_na, beds_na, review_scores_rating_na, review_scores_accuracy_na, review_scores_cleanliness_na, review_scores_checkin_na, review_scores_communication_na, review_scores_location_na, review_scores_value_na)\n",
       "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_f9fb...\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assemblerInputs = oheOutputCols ++ numericCols\n",
    "val vecAssembler = new VectorAssembler()\n",
    "  .setInputCols(assemblerInputs)\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Maintenant nous avons tous les features pour construire le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_d9aa8d73cc1b\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "  .setLabelCol(\"price\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0|109.20433611042972|\n",
      "|(98,[0,3,6,22,43,...| 45.0| 39.69910013655317|\n",
      "|(98,[0,3,6,22,43,...| 70.0| 44.60628120705951|\n",
      "|(98,[0,3,6,12,42,...|128.0|-41.91602285458066|\n",
      "|(98,[0,3,6,12,43,...|159.0|106.31505350096995|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_8adf69b2bb91, oneHotEncoder_1aa4a8e7119c, VectorAssembler: uid=vecAssembler_f9fb04a610f7, handleInvalid=error, numInputCols=33, linReg_d9aa8d73cc1b)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_c57c6a047f24\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_c57c6a047f24\n",
       "predDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellatio...\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val stages = Array(stringIndexer, oheEncoder, vecAssembler,  lr)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(stages)\n",
    "\n",
    "val pipelineModel = pipeline.fit(trainDF)\n",
    "val predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 : RFormula\n",
    "* Au lieu de spécifier manuellement quelles colonnes sont catégoriques pour StringIndexer et OneHotEncoder, RFormula peut le faire automatiquement pour vous [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.RFormula).\n",
    "* Avec RFormula, si vous avez des colonnes de type String, il la traite comme un *attribut* catégorique et crée les indexes et le one hot encoding pour nous. Sinon, il laisse tel quel. Ensuite, il combine toutes les features encodées et  numériques en un seul vecteur, appelé «features»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.RFormula\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rFormula: org.apache.spark.ml.feature.RFormula = RFormula: uid=rFormula_7d8c6db5b56f, formula = price ~ .\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val rFormula = new RFormula()\n",
    "  .setFormula(\"price ~ .\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"price\")\n",
    "  .setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,7,23,4...| 85.0|109.20433653512737|\n",
      "|(98,[0,3,6,7,23,4...| 45.0| 39.69909986710991|\n",
      "|(98,[0,3,6,7,23,4...| 70.0|  44.6062804524845|\n",
      "|(98,[0,3,6,7,13,4...|128.0| -41.9160226271124|\n",
      "|(98,[0,3,6,7,13,4...|159.0|106.31505335881957|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_2a199c55cc36\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_2a199c55cc36\n",
       "predDF: org.apache.spark.sql.DataFrame = [host_is_superhost: string, cancellation_policy: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(rFormula, lr))\n",
    "\n",
    "val pipelineModel = pipeline.fit(trainDF)\n",
    "val predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE est égale à 97.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "regressionEvaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_04344a9f6760, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 97.7482812471497\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val regressionEvaluator = new RegressionEvaluator()\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"price\")\n",
    "  .setMetricName(\"rmse\")\n",
    "\n",
    "val rmse = regressionEvaluator.evaluate(predDF)\n",
    "println(f\"RMSE est égale à $rmse%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2** représente le coefficient de l'adéquation des valeurs par rapport aux valeurs d'origine. La valeur de 0 à 1 interprétée comme des pourcentages. Plus la valeur est élevée, meilleur est le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R2 = 1- \\frac{\\sum_{i=1}^{n}(y_{i}-{\\hat{y_{i}}})^{2}}{\\sum_{i=1}^{n}(y_{i}-{\\bar{y}})^{2}} = 1 - \\frac{RMSE^2}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 est égale à 0.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r2: Double = 0.5113945481435695\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "println(f\"R2 est égale à $r2%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelinePath: String = datasets/sf-airbnb/lr-pipeline-model\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelinePath = \"datasets/sf-airbnb/lr-pipeline-model\"\n",
    "pipelineModel.write.overwrite().save(pipelinePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
